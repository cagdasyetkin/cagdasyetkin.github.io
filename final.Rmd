---

---

<b>2018.27.02 </b>
<style> body { text-align: justify} </style>

###I analyze a large dataset of commercial records produced by Assyrian merchants in the 19th Century BCE...



[this is how the language sounded like...](https://soundcloud.com/user444756202/the-epic-of-gilgames-standard-version-tablet-xi-lines-1-163-read-by-karl-hecker "from Gilgamesh")


<b>2018.01.03 </b>

###And this is how it looked like:

<img src="sample.png" alt="sample" height="200" width="300">


<b>2018.02.03 </b>


###Some smart people decrypted these languages. For example

nu ninda en e-iz-za-te-ni </n>
<b>wa-a-tar</b>-ma e-ku-ut-te-ni

Translation:

"Now you will eat bread and drink water"

wa-a-tar means water. ninda means bread. Quite familiar isn't it?


###Our data looks like this:

This is a txt file.


<img src="sample2.png" alt="data" height="300" width="500">


But how did I get here? It started with a tweet by Gabor Bekes:


<img src="tweetgabor.png" alt="gabortweet" height="300" width="500">


I contacted the authors of this academic paper from the University of Virginia. They were so kind and provided me these text files:


<img src="kerememail.png" alt="kerememail" height="300" width="500">




Archeologists have escavated these tablets during the past 200 years.

###An escavated tablet looks like this:

<img src="sample3.png" alt="tablet" height="200" width="200">



###These people lived 4000 years ago in the Middle East and the Asia Minor

<b>2018.03.05 </b>

We will start by loading our documents. We have many unwanted words and phrases. Some simple regular expressions will help us.

```{r, message=FALSE, warning=FALSE}
#we have a collection of 4 documents
library(dplyr)
oare1 <- readLines("OARE_01.txt") %>%
  data_frame(file = 'OARE_1') %>%
  rename('word' = '.') 

oare2 <- readLines("OARE_02.txt") %>%
  data_frame(file = 'OARE_2') %>%
  rename('word' = '.') 

oare3 <- readLines("OARE_03.txt") %>%
  data_frame(file = 'OARE_3') %>%
  rename('word' = '.') 

oare4 <- readLines("OARE_04.txt") %>%
  data_frame(file = 'OARE_4') %>%
  rename('word' = '.') 

txt <- bind_rows(list(oare1, oare2, oare3, oare4)) 
colnames(txt) <- c("text", "title")

#see the lines give the information regarding the Creator of the record
head(txt[grep('^Creator', txt$text),], 3)

#removing them
txt <- txt %>%
  anti_join(txt[grep('^Creator', txt$text),])
```

This is one way of cleaning such unwanted words... We can create a list of them.

<b>2018.03.06 </b>

This is how the list of unwanted words look like. We remove them just like we did above

```{r, message=FALSE, warning=FALSE}
toMatch <- c("^Primary", "^Translation", "^logosyllabic", "^Epistolary", "^Formul", "^Creator", 
             "^Editor", "^List", "^Introductory","^Publication", "^Physical" ,"^Chantre", 
             
             "^German", "^French ", "^Topic", "^“Kuzuoğlu", "^Body", "^Kayseri ",
             "^Periods", "^Kültepe", "^Karaduman", "^Kültepe", "Adana","^Seizure", 
             "^Bılgıç", "^First Topic", "^AnOr", "^Garelli", "^Liège", "^Prag",
             "^Ulshöfer", "^Burrill", "^Burton", "^Cole ", "^Eilsberger", "^hellbraunes",
             "^Stratford", "^Rendell", "^Schaeffer", "^Struwe", "^Larsen",
             
             "^AAA", "^A ", "^Ac.", "^ATHE", "^BIN", "^AKT", "^AO", "OARE", "^     ", "^KTB ",
             "^C ", "^CCT", "^CKAS", "^CTMMA", "^EL ", "^H.K. ", "^ICK ", "^JCS ", "^kt ",
             "^KTH ", "^KTS ", "^KUG ", "^¢", "^LB ", "^OIP ", "^POAT ", "^RA ", "^TC ", "^TMH ",
             "^TTC ", "^VS ", "^YBC ")


matches <- unique (grep(paste(toMatch,collapse="|"), 
                        txt$text, value=TRUE))

#removing them
txt <- txt %>%
  anti_join(data.frame(text = matches)) 

original_text <- txt # keep an untouched copy

#take a look at one commercial record now
txt[148:155, 1]

```

Here we are reading a business letter from a guy called Abela to Idnaya. Just a quick look makes it clear that these people were not savages at all. Freight charges, clearing a debt on behalf of another entity and stamps (seals) are all in place. 

The paragraph is almost clean. Next, we need to remove the numbers.


<b>2018.03.07 </b>

```{r, message=FALSE, warning=FALSE}
# Remove numbers using removeNumber function from tm library
library(tm)

txt$text <- txt$text %>%
  removeNumbers() %>%
  removePunctuation() %>% #in fact unnest_tokens function is handling this
  stripWhitespace()

```

Now let's have a quick look at the word frequencies

```{r, message=FALSE, fig.width=7,fig.height=4}
library(tidytext)
library(ggplot2)
library(ggthemes)

txt %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  count(word, sort = TRUE) %>%
  filter(n > 150) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) + geom_col() + xlab(NULL) + coord_flip() +
  ggtitle("Word Frequency")
  
```

It looks like these people were sending and receiving textiles, tin and copper. Transportation was mostly depending on donkeys and the payments were in silvers most of the time. The units of measurement (like the weight) were shekels, minas and talents. Example: Abele sends 5 minas of textile to Ibalaya for 20 shekels of silver, on behalf of Iddin-Istar.


<b>2018.03.08 </b>

Looking at the frequency plot again, what does **pusuken** mean? It is repeating more than 500 times...

```{r}
library(stringr)
head(original_text[str_detect(original_text$text, "Pušu-ken"), 1], 5)
```

A family business! It turns out Pushuken was the most famous businessman. Probably he was the leader of a powerful family controlling most of the trade happening in the region. His sons were handling the transactions. Assurnada and Assurmalik are the other two notable tradesmen coming out from the frequency table.

Another frequent word is 'son'. Let's find out why.

```{r}
tail(original_text[str_detect(original_text$text, "son"), 1], 5)
```

son of Assurnada, son of Inahili, son of Iddin... Of course, these are family names. That's why it repeats a lot.

<b>2018.03.10 </b>

Now we have a fairly good understanding of the most frequent words. Let's get a big picture of the ancient cities mentioned in our text. 

Ancient city names are easily available online. We will join them with our text. Our objective is to see how many times these cities are mentioned in our text and visualize them.

```{r, message=FALSE, warning=FALSE, echo=FALSE}
#Information from archeological findings
known_cities <- data.frame(cityName = c("hanaknak", "hattus", "hurama", "kanesh", "karahna", 
                                        "qattara", "salatuwar", "samuha", "tapaggas", 
                                        "timelkiya", "ulama", "unipsum", "wahsusana", "zimishuna"), 
                  type = 'known',
                  modern_name = c("Cankiri", "Bogazkoy", "Kusura", "Kultepe", "Sulusaray", 
                                  "Tell al-Rimah", "Mersin", "Kayapinar", "MasatHoyuk", "Telmessos",
                                  "Silifke", "Tünip", "Cyprus", "Samsun"),
                  latitude = c(40.6002, 40.018499926, 38.360937, 38.851165, 39.986138,
                               36.152551, 36.816874, 39.319128, 40.0854, 36.618181,
                               36.318340, 35.1725, 35.103410, 41.280517),
                  longitude = c(33.6162, 34.60916423, 30.230104, 35.635486, 36.093526,
                                42.265761, 34.687688, 36.309621, 35.4544, 29.117308,
                                33.869491, 36.2359, 33.338150, 36.327963)
                  )  
#lost city coordinates are estimates from the article
lost_cities <- data.frame(cityName = c("durhumit", "hahhum", "kuburnat", "mamma", "ninassa",
                                       "purushaddum", "sinahuttum", "suppiluliya", "tuhpiya",
                                       "washaniya", "zalpa"), 
                 type = 'lost',
                 latitude = c(40.725, 38.126, 39.748, 38.053, 38.714, 40.308, 40.056, 40.062,
                              39.787, 39.189, 37.704),
                 longitude = c(35.178, 37.776, 35.919, 36.301, 34.256, 33.379, 34.933, 34.683,
                               35.274, 34.221, 37.347))

cities <- bind_rows(known_cities, lost_cities)

```


```{r}
library(tidyr)

cities <- cities %>%
  left_join(original_text %>%
                unnest_tokens(word, text) %>%
                filter(word %in% tolower(cities$cityName)) %>%
                count(word, sort = TRUE) %>%
                rename(cityName = word),
            
            by = 'cityName')

cities$n <- replace_na(cities$n, 0)
```

```{r}
cities %>%
  select(cityName, latitude, longitude, n) %>%
  rename(Occurance_In_Text = n) %>%
  arrange(desc(Occurance_In_Text)) %>%
  head() %>%
  pander::pander()

#cities to plot
cities_in_text <- cities %>%
  filter(n > 0)
```

The city of Kanesh is our biggest hub. Probably a capital city. Our famous Pusheken family must have lived here.

```{r, message=FALSE, warning=FALSE, fig.width=7,fig.height=4}
library(ggmap)
library(ggthemes)

#setup map borders and select your map style
lat <- cities$latitude
long <- cities$longitude
bbox <- make_bbox(long,lat,f=0.5)
b <- get_map(bbox,maptype="watercolor",source="google")

#plot the cities mentioned in the text
ggmap(b) + geom_point(data = cities_in_text, 
           aes(longitude, latitude, size=n),alpha=0.2, show.legend = F) +
           scale_size_continuous(range = c(3, 10)) +
  
           labs(x = "Longitude", y = "Latitude",
           title="City Locations / Size = Occurance in Text", color = "Type") +
  
           geom_text(data = cities_in_text, aes(longitude, latitude, label = cityName), 
                     check_overlap = TRUE) + #nudge_y = -0.5) +
theme_gdocs()
```

They were using the sea ports in Timelkiya and Ulama. So we are talking about some complicated trade network here. Probably with Pharaohs of ancient Egypt, Phonecians and Mycenaeans.

<b>2018.03.11 </b>

We know the most frequent words in these documents. How about the most important ones? We can try tfidf here.
```{r}
oare_words <- txt %>%
  unnest_tokens(word, text) %>%
  count(title, word, sort = TRUE) %>%
  ungroup()

total_words <- oare_words %>% 
  group_by(title) %>% 
  summarize(total = sum(n))

oare_words <- left_join(oare_words, total_words, by = 'title')
```

```{r, message=FALSE, warning=FALSE}
oare_stop_words <- data.frame(word = c("gkt", "sec", "oaa", "ca", "g"), lexicon = 'oare')

oare_words <- oare_words %>%
  anti_join(oare_stop_words) %>%
  bind_tf_idf(word, title, n)

oare_words %>%
  select(-total) %>%
  arrange(desc(tf_idf)) %>%
  head(10) %>%
  select(title, word, n, tf_idf)
```

```{r, fig.height=6, message=FALSE}
oare_words %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(title) %>% 
  top_n(5) %>% 
  ungroup %>%
  ggplot(aes(word, tf_idf, fill = title)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~title, ncol = 2, scales = "free") +
  coord_flip() + theme_fivethirtyeight()
```




