---

---

<b>2018.27.02 </b>
<style> body { text-align: justify} </style>

###I analyze a large dataset of commercial records produced by Assyrian merchants in the 19th Century BCE...



[this is how the language sounded like...](https://soundcloud.com/user444756202/the-epic-of-gilgames-standard-version-tablet-xi-lines-1-163-read-by-karl-hecker "from Gilgamesh")


<b>2018.01.03 </b>

###And this is how it looked like:

![](sample.png)

<b>2018.02.03 </b>


###Some smart people decrypted these languages. For example

nu ninda en e-iz-za-te-ni </n>
<b>wa-a-tar</b>-ma e-ku-ut-te-ni

Translation:

"Now you will eat bread and drink water"

wa-a-tar means water. ninda means bread. Quite familiar isn't it?


###Records in our database look like this:

![](sample2.png)





###An escavated tablet looks like this:

<img src="sample3.png" alt="tablet" height="420" width="420">



###These people lived 4000 years ago. And this is where they lived:

![](sample4.jpg)



<b>2018.03.05 </b>

When we load the data, we will need to get rid of many words and phrases. Some simple regular expressions will help us. For example:

```{r, message=FALSE, warning=FALSE}
#Load the sample data
library(dplyr)
txt <- readLines("OARE_01.txt") %>%
  data_frame(file = 'OARE_1') %>%
  rename('text' = '.') 

#see the lines give the information regarding the Creator of the record
head(txt[grep('^Creator', txt$text),], 3)

#removing them
txt <- txt %>%
  anti_join(txt[grep('^Creator', txt$word),])
```

This is one way of cleaning such unwanted words... We can create a list of them.

<b>2018.03.06 </b>

Now we will make a list of unwanted words and remove them just like we did above

```{r, message=FALSE, warning=FALSE}
toMatch <- c("^Primary", "^Translation", "^logosyllabic", "^Epistolary", "^Formul", "OARE",
             "^Publication", "^Physical", "^German", "^AAA", "^Topic", "^Ac.", "^“Kuzuoğlu",
             "^Periods", "^Kültepe", "^Karaduman", "^Kültepe", "Adana", "^AKT", "^Introductory",
             "^Seizure", "^Bılgıç", "^First Topic", "^AnOr", "^AO", "^Garelli", "^ATHE", "^BIN",
             "^Ulshöfer", "^Burrill", "^Burton", "^Creator", "^A ", "^     ", "^Editor", "^List")

matches <- unique (grep(paste(toMatch,collapse="|"), 
                        txt$text, value=TRUE))

OARE_stop_words <- data.frame(text = matches, lexicon = 'OARE')

#removing them
txt <- txt %>%
  anti_join(OARE_stop_words)

#take a look at one commercial record now

txt[148:155, 1]

```

Here we are reading a business letter from a guy called Abela to Idnaya. Just a quick look makes it clear that these people were not savages at all. Freight charges, clearing a debt on behalf of another entity and stamps (seals) are all in place. 

The paragraph is almost clean. Next, we need to remove the numbers.


<b>2018.03.07 </b>

```{r, message=FALSE, warning=FALSE}
# Remove numbers using removeNumber function from tm library
library(tm)

txt$text <- txt$text %>%
  removeNumbers() %>%
  removePunctuation() %>%
  stripWhitespace()

```

Now let's have a quick look at the word frequencies

```{r, message=FALSE, fig.width=7,fig.height=4}
library(tidytext)
library(ggplot2)
library(ggthemes)

txt %>%
  unnest_tokens(word, text) %>%
  anti_join(stop_words) %>%
  count(word, sort = TRUE) %>%
  filter(n > 100) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) + geom_col() + xlab(NULL) + coord_flip() +
  ggtitle("OARE_01") +
  theme_fivethirtyeight()
  
```

It looks like these people were sending and receiving textiles, tin and copper. They were paying with silver most of the time. The units of measurement (like the weight) were shekels and minas. Example: Abele sends 5 minas of textile to Ibalaya for 20 shekels of silver.



