<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title></title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/paper.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-4.5.0/css/font-awesome.min.css" rel="stylesheet" />

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="main.css" type="text/css" />

</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 64px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 69px;
  margin-top: -69px;
}

.section h2 {
  padding-top: 69px;
  margin-top: -69px;
}
.section h3 {
  padding-top: 69px;
  margin-top: -69px;
}
.section h4 {
  padding-top: 69px;
  margin-top: -69px;
}
.section h5 {
  padding-top: 69px;
  margin-top: -69px;
}
.section h6 {
  padding-top: 69px;
  margin-top: -69px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Cagdas Yetkin</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="final.html">final</a>
</li>
<li>
  <a href="textdata.html">Textdata</a>
</li>
<li>
  <a href="AirBnB.html">AirBnB</a>
</li>
<li>
  <a href="mental.html">Mental</a>
</li>
<li>
  <a href="ml.html">Machine Leaning</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="http://github.com/cagdasyetkin">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="https://twitter.com/cagdasyetkin">
    <span class="fa fa-twitter fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="https://www.linkedin.com/in/cagdasyetkin/">
    <span class="fa fa-linkedin fa-lg"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">




</div>


<hr />
<hr />
<p><img src="images/mental_images/mychimp.PNG" alt="chimp" height="450" width="450"></p>
<p>Can we do better than a Dart Throwing Chimpanzee? It is said by the experts that a dart throwing chimp can hit the target 60% of the time!</p>
<div id="chapter-1" class="section level2">
<h2>Chapter 1</h2>
<div id="going-mental-in-the-tech-industry" class="section level3">
<h3>Going Mental in the Tech Industry…</h3>
<p>In this task we are going to predict mental illness for workers in the tech sector. The data comes from Kaggle. The variable to predict is ‘treatment’.</p>
<pre class="r"><code>library(ggplot2)
library(data.table)
library(caret)
library(glmnet)
library(ROCR)
library(dplyr)
library(ggthemes)

knitr::opts_chunk$set(warning = FALSE)
knitr::opts_chunk$set(message = FALSE)</code></pre>
</div>
<div id="predicting-mental-health-problems-in-the-tech-sector" class="section level3">
<h3>Predicting mental health problems in the tech sector</h3>
<pre class="r"><code>data &lt;- fread(&quot;data/mentaldata/survey_cleaned.csv&quot;)
data &lt;- data[ ,c(&quot;comments&quot;, &quot;state&quot;,&quot;work_interfere&quot;) := NULL]
data[, age := as.numeric(age)]
data[ , treatment := factor(treatment, levels = c(&quot;Yes&quot;, &quot;No&quot;))]</code></pre>
<p>Explore some predictors that can be used to predict treatment</p>
<pre class="r"><code>names(data)</code></pre>
<pre><code>##  [1] &quot;Timestamp&quot;                 &quot;age&quot;                      
##  [3] &quot;gender&quot;                    &quot;country&quot;                  
##  [5] &quot;self_employed&quot;             &quot;family_history&quot;           
##  [7] &quot;treatment&quot;                 &quot;no_employees&quot;             
##  [9] &quot;remote_work&quot;               &quot;tech_company&quot;             
## [11] &quot;benefits&quot;                  &quot;care_options&quot;             
## [13] &quot;wellness_program&quot;          &quot;seek_help&quot;                
## [15] &quot;anonymity&quot;                 &quot;leave&quot;                    
## [17] &quot;mental_health_consequence&quot; &quot;phys_health_consequence&quot;  
## [19] &quot;coworkers&quot;                 &quot;supervisor&quot;               
## [21] &quot;mental_health_interview&quot;   &quot;phys_health_interview&quot;    
## [23] &quot;mental_vs_physical&quot;        &quot;obs_consequence&quot;</code></pre>
<pre class="r"><code>glimpse(data)</code></pre>
<pre><code>## Observations: 1,256
## Variables: 24
## $ Timestamp                 &lt;chr&gt; &quot;2014-08-27 11:29:31&quot;, &quot;2014-08-27 1...
## $ age                       &lt;dbl&gt; 37, 44, 32, 31, 31, 33, 35, 39, 42, ...
## $ gender                    &lt;chr&gt; &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;male&quot;, &quot;mal...
## $ country                   &lt;chr&gt; &quot;United States&quot;, &quot;United States&quot;, &quot;C...
## $ self_employed             &lt;chr&gt; &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, &quot;&quot;, ...
## $ family_history            &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;...
## $ treatment                 &lt;fct&gt; Yes, No, No, Yes, No, No, Yes, No, Y...
## $ no_employees              &lt;chr&gt; &quot;6-25&quot;, &quot;More than 1000&quot;, &quot;6-25&quot;, &quot;2...
## $ remote_work               &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;,...
## $ tech_company              &lt;chr&gt; &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Y...
## $ benefits                  &lt;chr&gt; &quot;Yes&quot;, &quot;Don&#39;t know&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Ye...
## $ care_options              &lt;chr&gt; &quot;Not sure&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;,...
## $ wellness_program          &lt;chr&gt; &quot;No&quot;, &quot;Don&#39;t know&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Don...
## $ seek_help                 &lt;chr&gt; &quot;Yes&quot;, &quot;Don&#39;t know&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Do...
## $ anonymity                 &lt;chr&gt; &quot;Yes&quot;, &quot;Don&#39;t know&quot;, &quot;Don&#39;t know&quot;, &quot;...
## $ leave                     &lt;chr&gt; &quot;Somewhat easy&quot;, &quot;Don&#39;t know&quot;, &quot;Some...
## $ mental_health_consequence &lt;chr&gt; &quot;No&quot;, &quot;Maybe&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;N...
## $ phys_health_consequence   &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;,...
## $ coworkers                 &lt;chr&gt; &quot;Some of them&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Some o...
## $ supervisor                &lt;chr&gt; &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Ye...
## $ mental_health_interview   &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Maybe&quot;, &quot;Yes&quot;, &quot;...
## $ phys_health_interview     &lt;chr&gt; &quot;Maybe&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Maybe&quot;, &quot;Yes&quot;...
## $ mental_vs_physical        &lt;chr&gt; &quot;Yes&quot;, &quot;Don&#39;t know&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Do...
## $ obs_consequence           &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;,...</code></pre>
<pre class="r"><code>sapply(data, function(x) sum(is.na(x)))</code></pre>
<pre><code>##                 Timestamp                       age 
##                         0                         0 
##                    gender                   country 
##                         0                         0 
##             self_employed            family_history 
##                         0                         0 
##                 treatment              no_employees 
##                         0                         0 
##               remote_work              tech_company 
##                         0                         0 
##                  benefits              care_options 
##                         0                         0 
##          wellness_program                 seek_help 
##                         0                         0 
##                 anonymity                     leave 
##                         0                         0 
## mental_health_consequence   phys_health_consequence 
##                         0                         0 
##                 coworkers                supervisor 
##                         0                         0 
##   mental_health_interview     phys_health_interview 
##                         0                         0 
##        mental_vs_physical           obs_consequence 
##                         0                         0</code></pre>
<pre class="r"><code>data$gender %&gt;% unique()</code></pre>
<pre><code>## [1] &quot;male&quot;   &quot;trans&quot;  &quot;female&quot;</code></pre>
<pre class="r"><code>data[, age_cat := cut(age, .(-Inf,20,35,65,Inf), labels = c(&#39;Kids&#39;,&#39;Young&#39;,&#39;Middle&#39;,&#39;Old&#39;))]</code></pre>
<pre class="r"><code>ggplot(data, aes(age_cat)) + geom_bar() + facet_grid(~gender) + theme_economist_white()</code></pre>
<p><img src="mental_files/figure-html/unnamed-chunk-7-1.png" width="576" /> This data is mostly about young males which is representative for the tech industry. But which one is seeking relatively more treatment? Males or females?</p>
<pre class="r"><code>ggplot(data, aes(x = treatment, group = gender)) + 
  geom_bar(aes(y = ..prop.., fill = factor(..x..)), stat=&quot;count&quot;) +
  geom_text(aes( label = scales::percent(..prop..),
                   y= ..prop.. ), stat= &quot;count&quot;, vjust = -.1) +
  scale_y_continuous(labels=scales::percent) +
  facet_grid(~gender) +
  labs(y = &quot;Percent&quot;, fill=&quot;Treatment&quot;) +
  theme_fivethirtyeight() + scale_fill_grey()</code></pre>
<p><img src="mental_files/figure-html/unnamed-chunk-8-1.png" width="576" /></p>
<p>I said males or females and the trans gender came out with a surprise</p>
<p>How about family history?</p>
<pre class="r"><code>ggplot(data, aes(x = treatment, group = family_history)) + 
  geom_bar(aes(y = ..prop.., fill = factor(..x..)), stat=&quot;count&quot;) +
    geom_text(aes( label = scales::percent(..prop..),
                   y= ..prop.. ), stat= &quot;count&quot;, vjust = -.1) +
  scale_y_continuous(labels=scales::percent) +
  facet_grid(~family_history) +
  labs(y = &quot;Percent&quot;, fill=&quot;Treatment&quot;) +
  theme_fivethirtyeight() + scale_fill_grey()</code></pre>
<p><img src="mental_files/figure-html/unnamed-chunk-9-1.png" width="576" /></p>
<p>Family really matters a big deal!</p>
<pre class="r"><code>ggplot(data, aes(x = treatment, group = age_cat)) + 
  geom_bar(aes(y = ..prop.., fill = factor(..x..)), stat=&quot;count&quot;) +
    geom_text(aes( label = scales::percent(..prop..),
                   y= ..prop.. ), stat= &quot;count&quot;, vjust = -.1) +
  scale_y_continuous(labels=scales::percent) +
  facet_grid(~age_cat) +
  labs(y = &quot;Percent&quot;, fill=&quot;Treatment&quot;) +
  theme_fivethirtyeight() + scale_fill_grey()</code></pre>
<p><img src="mental_files/figure-html/unnamed-chunk-10-1.png" width="576" /></p>
<p>We can see a pattern here. Older the age category, more the treatment. Similarly younger age categories are seeking less and less treatment.</p>
<p>Why do I have 100% in the oldest age category?</p>
<pre class="r"><code>data %&gt;% filter(age &gt; 65) %&gt;% select(age_cat, gender, family_history, treatment)</code></pre>
<pre><code>##   age_cat gender family_history treatment
## 1     Old   male             No       Yes
## 2     Old  trans            Yes       Yes
## 3     Old   male            Yes       Yes</code></pre>
<p>It turns out there are only 3 observations over there. As we have seen above the dominant age category is 20-35 years old.</p>
<pre class="r"><code>ggplot(data, aes(x = treatment, group = seek_help)) + 
  geom_bar(aes(y = ..prop.., fill = factor(..x..)), stat=&quot;count&quot;) +
  geom_text(aes( label = scales::percent(..prop..),
                   y= ..prop.. ), stat= &quot;count&quot;, vjust = -.1) +
  labs(y = &quot;Percent&quot;, fill=&quot;treatment&quot;) +
  facet_grid(~seek_help) +
  scale_y_continuous(labels = scales::percent) +
  theme_fivethirtyeight() + scale_fill_grey()</code></pre>
<p><img src="mental_files/figure-html/unnamed-chunk-12-1.png" width="576" /></p>
<p>There can be some evidence for the higher probability of ending up in a treatment for the people who are aware of their employer’s services regarding mental issues</p>
<pre class="r"><code>ggplot(data, aes(x = treatment, group = leave)) + 
  geom_bar(aes(y = ..prop.., fill = factor(..x..)), stat=&quot;count&quot;) +
  geom_text(aes( label = scales::percent(..prop..),
                   y= ..prop.. ), stat= &quot;count&quot;, vjust = -.2) +
  labs(y = &quot;Percent&quot;, fill=&quot;treatment&quot;) +
  facet_grid(~leave) +
  scale_y_continuous(labels = scales::percent) +
  theme_fivethirtyeight() + scale_fill_grey()</code></pre>
<p><img src="mental_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>People who indicate that there is some difficulty on getting a leave for mental issue reasons might giving us signal.</p>
<p>Partitioning the data to 70% training and 30% test samples.</p>
<pre class="r"><code>set.seed(123)
my_ratio &lt;- 0.7
train_indices &lt;- createDataPartition(y = data[[&quot;treatment&quot;]],
                                     times = 1,
                                     p = my_ratio,
                                     list = FALSE)

data_train &lt;- data[train_indices, ]
data_test  &lt;- data[-train_indices, ]</code></pre>
<p>Build models with glmnet and rpart that predict the binary outcome of treatment. Using cross-validation on the training set and AUC as a selection measure.</p>
<pre class="r"><code>train_control &lt;- trainControl(method = &quot;cv&quot;,
                              number = 5,
                              classProbs = TRUE,
                              summaryFunction = twoClassSummary)

tune_grid &lt;- expand.grid(&quot;alpha&quot; = c(0, 1),
                         &quot;lambda&quot; = seq(0.1, 0.14, 0.01))

set.seed(123)
glmnet_model &lt;- train(treatment ~ age_cat + seek_help + benefits + supervisor +
                                  family_history + leave + gender,
                      data = data_train,
                      method = &quot;glmnet&quot;,
                      preProcess = c(&quot;center&quot;, &quot;scale&quot;),
                      trControl = train_control,
                      tuneGrid = tune_grid,
                      metric = &quot;ROC&quot;) 
glmnet_model</code></pre>
<pre><code>## glmnet 
## 
## 880 samples
##   7 predictor
##   2 classes: &#39;Yes&#39;, &#39;No&#39; 
## 
## Pre-processing: centered (16), scaled (16) 
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 704, 705, 704, 703, 704 
## Resampling results across tuning parameters:
## 
##   alpha  lambda  ROC        Sens       Spec     
##   0      0.10    0.7448183  0.6576864  0.7178422
##   0      0.11    0.7452348  0.6576864  0.7178422
##   0      0.12    0.7452360  0.6576864  0.7178422
##   0      0.13    0.7454450  0.6554392  0.7155434
##   0      0.14    0.7453153  0.6531920  0.7132706
##   1      0.10    0.6904544  0.5540347  0.8049634
##   1      0.11    0.6840193  0.5540347  0.8049634
##   1      0.12    0.6794991  0.5540347  0.8049634
##   1      0.13    0.6794991  0.5540347  0.8049634
##   1      0.14    0.6794991  0.5540347  0.8049634
## 
## ROC was used to select the optimal model using the largest value.
## The final values used for the model were alpha = 0 and lambda = 0.13.</code></pre>
<pre class="r"><code>trctrl &lt;- trainControl(method = &quot;cv&quot;, 
                       number = 5, 
                       classProbs = T, 
                       #verboseIter = T, 
                       summaryFunction = twoClassSummary)

tune_grid &lt;- data.frame(cp=seq(0.0001, 0.01, 0.001))

set.seed(123)
treeCPModel &lt;- train(treatment ~ age_cat + seek_help + benefits + supervisor +
                                 family_history + leave + gender,
                     data = data_train, 
                     method = &quot;rpart&quot;,
                     trControl = trctrl,
                     preProcess = c(&quot;center&quot;, &quot;scale&quot;),
                     tuneGrid = tune_grid,
                     metric = &#39;ROC&#39;)
                  
treeCPModel</code></pre>
<pre><code>## CART 
## 
## 880 samples
##   7 predictor
##   2 classes: &#39;Yes&#39;, &#39;No&#39; 
## 
## Pre-processing: centered (16), scaled (16) 
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 704, 705, 704, 703, 704 
## Resampling results across tuning parameters:
## 
##   cp      ROC        Sens       Spec     
##   0.0001  0.7156373  0.6800817  0.6422936
##   0.0011  0.7139538  0.6935904  0.6376959
##   0.0021  0.7153335  0.6936415  0.6491641
##   0.0031  0.7194024  0.7072268  0.6422675
##   0.0041  0.7194024  0.7072268  0.6422675
##   0.0051  0.7191620  0.6734168  0.7039446
##   0.0061  0.7193958  0.6688968  0.7085162
##   0.0071  0.7193958  0.6688968  0.7085162
##   0.0081  0.7193958  0.6688968  0.7085162
##   0.0091  0.7051818  0.6281920  0.7268548
## 
## ROC was used to select the optimal model using the largest value.
## The final value used for the model was cp = 0.0041.</code></pre>
<p>Now, compare models based on their predictive performance based on the cross-validation information. We can just use the mean AUC to select the best model.</p>
</div>
<div id="glmnet-performs-better-than-the-tree-74.5-vs-71.9-area-under-the-curve." class="section level3">
<h3>Glmnet performs better than the tree: 74.5% vs 71.9% area under the curve.</h3>
<p>Now evaluating the better performing model on the test set, drawing a ROC curve and interpreting the AUC.</p>
<pre class="r"><code>test_prediction &lt;- predict.train(glmnet_model, newdata = data_test)
test_truth &lt;- data_test[[&quot;treatment&quot;]]</code></pre>
<p>There are 4 cases:</p>
<ul>
<li>true positives: those that are positive in reality and we correctly predict them to be positive</li>
<li>false positives: those that are negative in reality and we falsely predict them to be positive</li>
<li>true negatives: those that are negative in reality and we correctly predict them to be negative</li>
<li>false negatives: those that are positive in reality and we falsely predict them to be negative</li>
</ul>
<pre class="r"><code>confusionMatrix(test_prediction, test_truth)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction Yes  No
##        Yes 132  64
##        No   58 122
##                                           
##                Accuracy : 0.6755          
##                  95% CI : (0.6256, 0.7226)
##     No Information Rate : 0.5053          
##     P-Value [Acc &gt; NIR] : 1.777e-11       
##                                           
##                   Kappa : 0.3508          
##  Mcnemar&#39;s Test P-Value : 0.6508          
##                                           
##             Sensitivity : 0.6947          
##             Specificity : 0.6559          
##          Pos Pred Value : 0.6735          
##          Neg Pred Value : 0.6778          
##              Prevalence : 0.5053          
##          Detection Rate : 0.3511          
##    Detection Prevalence : 0.5213          
##       Balanced Accuracy : 0.6753          
##                                           
##        &#39;Positive&#39; Class : Yes             
## </code></pre>
<p>The various types of errors have to be examined and we have to decide based on them.</p>
<pre class="r"><code># obtain probabilities instead of binary predictions
test_prediction_probs &lt;- predict.train(glmnet_model, 
                                       newdata = data_test, 
                                       type = &quot;prob&quot;)
head(test_prediction_probs)</code></pre>
<pre><code>##         Yes        No
## 1 0.6769227 0.3230773
## 2 0.6396976 0.3603024
## 3 0.3119165 0.6880835
## 4 0.4018016 0.5981984
## 5 0.6274032 0.3725968
## 6 0.7856110 0.2143890</code></pre>
<pre class="r"><code>summary(test_prediction_probs$Yes)</code></pre>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##  0.2779  0.3640  0.5072  0.5149  0.6580  0.8601</code></pre>
<p>By default, predict.train uses the 50% threshold for prediction. We can try 45% also:</p>
<pre class="r"><code>test_prediction_v2 &lt;- ifelse(test_prediction_probs$Yes &gt; 0.45, &quot;Yes&quot;, &quot;No&quot;)
test_prediction_v2 &lt;- factor(test_prediction_v2, levels = c(&quot;Yes&quot;, &quot;No&quot;))
confusionMatrix(test_prediction_v2, test_truth)</code></pre>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction Yes  No
##        Yes 150  82
##        No   40 104
##                                           
##                Accuracy : 0.6755          
##                  95% CI : (0.6256, 0.7226)
##     No Information Rate : 0.5053          
##     P-Value [Acc &gt; NIR] : 1.777e-11       
##                                           
##                   Kappa : 0.3494          
##  Mcnemar&#39;s Test P-Value : 0.0002057       
##                                           
##             Sensitivity : 0.7895          
##             Specificity : 0.5591          
##          Pos Pred Value : 0.6466          
##          Neg Pred Value : 0.7222          
##              Prevalence : 0.5053          
##          Detection Rate : 0.3989          
##    Detection Prevalence : 0.6170          
##       Balanced Accuracy : 0.6743          
##                                           
##        &#39;Positive&#39; Class : Yes             
## </code></pre>
<p>We lowered our false negatives a bit by using the threshold of 0.45 above. Lets do a search for threshold.</p>
</div>
<div id="varying-thresholds" class="section level3">
<h3>Varying thresholds</h3>
<p>If we increase the threshold for predicting something to be positive: we will have less and less cases that we label as positive. Both of those that are positive in reality and of those that are negative. Thus, both the true positives and the false positives increase.</p>
<pre class="r"><code>thresholds &lt;- seq(0.3, 0.6, by = 0.05)

for (thr in thresholds) {
  test_prediction &lt;- ifelse(test_prediction_probs$Yes &gt; thr, &quot;Yes&quot;, &quot;No&quot;)
  test_prediction &lt;- factor(test_prediction, levels = c(&quot;Yes&quot;, &quot;No&quot;))
  print(paste(&quot;Threshold:&quot;, thr))
  print(confusionMatrix(test_prediction, test_truth)[[&quot;table&quot;]])
} </code></pre>
<pre><code>## [1] &quot;Threshold: 0.3&quot;
##           Reference
## Prediction Yes  No
##        Yes 189 172
##        No    1  14
## [1] &quot;Threshold: 0.35&quot;
##           Reference
## Prediction Yes  No
##        Yes 170 125
##        No   20  61
## [1] &quot;Threshold: 0.4&quot;
##           Reference
## Prediction Yes  No
##        Yes 161  98
##        No   29  88
## [1] &quot;Threshold: 0.45&quot;
##           Reference
## Prediction Yes  No
##        Yes 150  82
##        No   40 104
## [1] &quot;Threshold: 0.5&quot;
##           Reference
## Prediction Yes  No
##        Yes 132  64
##        No   58 122
## [1] &quot;Threshold: 0.55&quot;
##           Reference
## Prediction Yes  No
##        Yes 109  46
##        No   81 140
## [1] &quot;Threshold: 0.6&quot;
##           Reference
## Prediction Yes  No
##        Yes  89  33
##        No  101 153</code></pre>
<p>What to choose then? I would choose .45 threshold because it is a balanced point for my business objectives.</p>
<p>My strategy is to minimize the situations where I predict no-treatment but in reality it is a yes. In those cases I do a terrible mistake. The outcome has a high cost for the company. I am not aware of the troubled people around. It is like sitting on a time bomb which we dont know it exists.</p>
<p>However, I cant lower my threshold too much also. In that case I would do too many false positives which has other kind of costs. It will impact the time and productivity negatively.</p>
<p>On the other hand, we can accept False Positives until to a centain extend. There is a sweet spot where they are not much costly compared to False Negatives. Just like some false fire alarms will not cause a lot of trouble.</p>
<p>After some scientific meditation, 0.45 will be the threshold I would use.</p>
At 0.45 Threshold Point I have:
<p>
<ol style="list-style-type: lower-alpha">
<li>78.9% True Positive Rate. That is TP/(TP+FN) &gt;&gt; 150/(150+40)
</p></li>
</ol>
<p>
<ol start="2" style="list-style-type: lower-alpha">
<li>44% False Positive Rate. That is FP/(FP+TN) &gt;&gt; 82/(82+104)
</p></li>
</ol>
<p>82 False Posities and 40 False Negatives.</p>
</div>
<div id="lets-see-the-roc-curve" class="section level3">
<h3>Lets see The ROC curve</h3>
<p>The ROC curve summarizes how a binary classifier performs “overall”, taking into accounts all possible thresholds. It shows the trade-off between true positive rate (a.k.a sensitivity, # true positives / # all positives) and the false positive rate (a.k.a 1 - specificity, # false positive / # negatives).</p>
<pre class="r"><code># a ggplot
# using prediction function from ROCR package
glmnet_prediction &lt;- prediction(test_prediction_probs$Yes,
                              data_test[[&quot;treatment&quot;]])
glmnet_perf &lt;- performance(glmnet_prediction, measure = &quot;tpr&quot;, x.measure = &quot;fpr&quot;)

glmnet_roc_df &lt;- data.table(
  model = &quot;glm&quot;,
  FPR = glmnet_perf@x.values[[1]],
  TPR = glmnet_perf@y.values[[1]],
  cutoff = glmnet_perf@alpha.values[[1]]
)

ggplot(glmnet_roc_df) +
  geom_line(aes(FPR, TPR, color = cutoff), size = 2) +
  geom_ribbon(aes(FPR, ymin = 0, ymax = TPR), alpha = 0.1) +
  geom_abline(intercept = 0, slope = 1,  linetype = &quot;dotted&quot;, col = &quot;black&quot;) +
  scale_y_continuous(limits = c(0, 1), breaks = seq(0, 1, .1)) +
  scale_x_continuous(limits = c(0, 1), breaks = seq(0, 1, .1)) +
  theme_fivethirtyeight() + scale_fill_grey() </code></pre>
<p><img src="mental_files/figure-html/unnamed-chunk-23-1.png" width="384" /></p>
</div>
<div id="auc" class="section level3">
<h3>AUC</h3>
<p>Higher AUC generally means better classification.</p>
<pre class="r"><code># calculate AUC
AUC &lt;- performance(glmnet_prediction, &quot;auc&quot;)@y.values[[1]]
print(AUC)</code></pre>
<pre><code>## [1] 0.7259904</code></pre>
<p>The colored field’s area in the plot is 0.725. A superb classifier would yield an area very close to 1. This is not one of the best models. However, it is better than flipping a coin. Or, it is better than a <strong>dart throwing chimpanzee</strong></p>
</div>
</div>
<div id="chapter-2" class="section level2">
<h2>Chapter 2</h2>
<div id="transformed-scores" class="section level3">
<h3>Transformed scores</h3>
<div id="the-application-of-calibration-in-prediction-tasks." class="section level4">
<h4>The application of Calibration in prediction tasks.</h4>
<p>This data is about patients taking an appointment from the doctors. They are labeled as ‘no-show’ when they have an appointment but never showed up. We will first predict this no-show behaviour and then do some calibration.</p>
<pre class="r"><code># https://www.kaggle.com/joniarroba/noshowappointments
data &lt;- fread(&quot;data/noshowdata/no-show-data.csv&quot;)
glimpse(data)</code></pre>
<pre><code>## Observations: 110,527
## Variables: 14
## $ PatientId      &lt;dbl&gt; 2.987250e+13, 5.589978e+14, 4.262962e+12, 8.679...
## $ AppointmentID  &lt;int&gt; 5642903, 5642503, 5642549, 5642828, 5642494, 56...
## $ Gender         &lt;chr&gt; &quot;F&quot;, &quot;M&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F...
## $ ScheduledDay   &lt;chr&gt; &quot;2016-04-29T18:38:08Z&quot;, &quot;2016-04-29T16:08:27Z&quot;,...
## $ AppointmentDay &lt;chr&gt; &quot;2016-04-29T00:00:00Z&quot;, &quot;2016-04-29T00:00:00Z&quot;,...
## $ Age            &lt;int&gt; 62, 56, 62, 8, 56, 76, 23, 39, 21, 19, 30, 29, ...
## $ Neighbourhood  &lt;chr&gt; &quot;JARDIM DA PENHA&quot;, &quot;JARDIM DA PENHA&quot;, &quot;MATA DA ...
## $ Scholarship    &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,...
## $ Hipertension   &lt;int&gt; 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ Diabetes       &lt;int&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ Alcoholism     &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ Handcap        &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...
## $ SMS_received   &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,...
## $ `No-show`      &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes...</code></pre>
<pre class="r"><code># some data cleaning
data[, c(&quot;PatientId&quot;, &quot;AppointmentID&quot;, &quot;Neighbourhood&quot;) := NULL]
setnames(data, 
         c(&quot;No-show&quot;, 
           &quot;Age&quot;, 
           &quot;Gender&quot;,
           &quot;ScheduledDay&quot;, 
           &quot;AppointmentDay&quot;,
           &quot;Scholarship&quot;,
           &quot;Hipertension&quot;,
           &quot;Diabetes&quot;,
           &quot;Alcoholism&quot;,
           &quot;Handcap&quot;,
           &quot;SMS_received&quot;), 
         c(&quot;no_show&quot;, 
           &quot;age&quot;, 
           &quot;gender&quot;, 
           &quot;scheduled_day&quot;, 
           &quot;appointment_day&quot;,
           &quot;scholarship&quot;,
           &quot;hypertension&quot;,
           &quot;diabetes&quot;,
           &quot;alcoholism&quot;,
           &quot;handicap&quot;,
           &quot;sms_received&quot;))
# clean up a little bit
data &lt;- data[age %between% c(0, 95)]
# for binary prediction with caret, the target variable must be a factor
data[, no_show := factor(no_show, levels = c(&quot;Yes&quot;, &quot;No&quot;))] #first one got to be YES, the positive
data[, no_show_num := ifelse(no_show == &quot;Yes&quot;, 1, 0)]
data[, handicap := ifelse(handicap &gt; 0, 1, 0)]

# create new variables
data[, scheduled_day := as.Date(scheduled_day)]
data[, appointment_day := as.Date(appointment_day)]
data[, days_since_scheduled := as.integer(appointment_day - scheduled_day)]
data &lt;- data[days_since_scheduled &gt; -1]</code></pre>
<pre class="r"><code>data[, no_show_num := NULL]

data[, days_category := cut(
  days_since_scheduled, 
  breaks = c(-1, 0, 1, 2, 5, 10, 30, Inf), 
  include.lowest = TRUE)]

data[, age_category := cut(age, 
                           breaks = seq(0, 100, by = 5), 
                           include.lowest = TRUE)]</code></pre>
<p>Now create a training and a test data and estimate a simple logistic regression to predict <code>no_show</code>.</p>
<pre class="r"><code>training_ratio &lt;- 0.5 
set.seed(1234)
train_indices &lt;- createDataPartition(y = data[[&quot;no_show&quot;]],
                                     times = 1,
                                     p = training_ratio,
                                     list = FALSE)
data_train &lt;- data[train_indices, ]
data_test &lt;- data[-train_indices, ]</code></pre>
<pre class="r"><code>train_control &lt;- trainControl(method = &quot;cv&quot;,
                              number = 5,
                              classProbs = TRUE)
set.seed(857)
glm_model &lt;- train(no_show ~ age_category + gender + days_since_scheduled,
                   method = &quot;glm&quot;,
                   data = data_train,
                   trControl = train_control)

test_prediction &lt;- predict.train(glm_model, newdata = data_test)
test_truth &lt;- data_test[[&quot;no_show&quot;]]</code></pre>
<pre class="r"><code># obtain probabilities instead of binary predictions
test_prediction_probs &lt;- predict.train(glm_model, 
                                       newdata = data_test, 
                                       type = &quot;prob&quot;)</code></pre>
<pre class="r"><code>summary(test_prediction_probs$Yes)</code></pre>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##  0.0939  0.1479  0.1829  0.2011  0.2265  0.9542</code></pre>
<pre class="r"><code>prediction &lt;- test_prediction_probs$Yes
prediction_sqrt &lt;- sqrt(prediction)
prediction_sq &lt;- prediction^2</code></pre>
<pre class="r"><code>rocr_prediction &lt;- prediction(prediction, test_truth)
rocr_sqrt &lt;- prediction(prediction_sqrt, test_truth)
rocr_sq &lt;- prediction(prediction_sq, test_truth)
# built-in plot method

plot(performance(rocr_sqrt, &quot;tpr&quot;, &quot;fpr&quot;), colorize = FALSE) 
plot(performance(rocr_sq, &quot;tpr&quot;, &quot;fpr&quot;), add = TRUE, colorize = FALSE) 
plot(performance(rocr_prediction, &quot;tpr&quot;, &quot;fpr&quot;), add = TRUE, colorize = FALSE) </code></pre>
<p><img src="mental_files/figure-html/unnamed-chunk-33-1.png" width="288" /></p>
<p>Area under the curves are the same for all three of them. Because it is the same model. Sensitivity and specificity have an inverse relationship. Increasing one would always decrease the other and Area Under the Curve remains the same.</p>
<pre class="r"><code># calculate AUC
AUC &lt;- performance(rocr_prediction, &quot;auc&quot;)@y.values[[1]]
AUC_sqrt &lt;- performance(rocr_sqrt, &quot;auc&quot;)@y.values[[1]]
AUC_sq &lt;- performance(rocr_sq, &quot;auc&quot;)@y.values[[1]]


AUC_results &lt;- c(AUC, AUC_sqrt, AUC_sq)
AUC_results</code></pre>
<pre><code>## [1] 0.6559295 0.6559295 0.6559295</code></pre>
</div>
</div>
<div id="calibration" class="section level3">
<h3>Calibration</h3>
<p>Can the scores produced by the model be regarded as probabilities? Let’s calculate the predicted and actual share of positive cases for groups of observations in the test set based on their predicted scores.</p>
<pre class="r"><code>truth_numeric &lt;- ifelse(test_truth == &quot;Yes&quot;, 1, 0)
score_glm &lt;- test_prediction_probs$Yes

summary(score_glm)</code></pre>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##  0.0939  0.1479  0.1829  0.2011  0.2265  0.9542</code></pre>
<pre class="r"><code>actual_vs_predicted &lt;- data.table(actual = truth_numeric,
                                  predicted = score_glm)

actual_vs_predicted[, score_category := cut(predicted,
                                    seq(0, 0.4, 0.1),
                                    include.lowest = TRUE)]
calibration &lt;- actual_vs_predicted[, .(mean_actual = mean(actual),
                                       mean_predicted = mean(predicted),
                                       num_obs = .N),
                                   keyby = .(score_category)]
ggplot(calibration,
       aes(x = mean_actual, y = mean_predicted, size = num_obs)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = &quot;dashed&quot;) +
  ylim(0, 1) + xlim(0, 1)</code></pre>
<p><img src="mental_files/figure-html/unnamed-chunk-36-1.png" width="480" /></p>
<p>This one is <strong>well calibrated</strong>. Predicted mean groups are similar to the actual means. It looks like a well calibrated classifier does not mean a perfect classifier.</p>
<p>Below we will see the calibration when we take squre root of the predictions. In general if we are following the 45 degree line it is a good sign for calibration.</p>
<pre class="r"><code>actual_vs_predicted &lt;- data.table(actual = truth_numeric,
                                  predicted = prediction_sqrt)

actual_vs_predicted[, score_category := cut(predicted,
                                    seq(0, 0.4, 0.05),
                                    include.lowest = TRUE)]
calibration &lt;- actual_vs_predicted[, .(mean_actual = mean(actual),
                                       mean_predicted = mean(predicted),
                                       num_obs = .N),
                                   keyby = .(score_category)]
ggplot(calibration,
       aes(x = mean_actual, y = mean_predicted, size = num_obs)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = &quot;dashed&quot;) +
  ylim(0, 1) + xlim(0, 1)</code></pre>
<p><img src="mental_files/figure-html/unnamed-chunk-37-1.png" width="480" /> Of course since we increased the values of our predictions they went up in the y axis, and the transformation was not a linear one.</p>
<p>We will see a similar effect below to the <strong>other</strong> direction.</p>
<pre class="r"><code>actual_vs_predicted &lt;- data.table(actual = truth_numeric,
                                  predicted = prediction_sq)

actual_vs_predicted[, score_category := cut(predicted,
                                    seq(0, 0.4, 0.05),
                                    include.lowest = TRUE)]
calibration &lt;- actual_vs_predicted[, .(mean_actual = mean(actual),
                                       mean_predicted = mean(predicted),
                                       num_obs = .N),
                                   keyby = .(score_category)]
ggplot(calibration,
       aes(x = mean_actual, y = mean_predicted, size = num_obs)) +
  geom_point() +
  geom_abline(intercept = 0, slope = 1, linetype = &quot;dashed&quot;) +
  ylim(0, 1) + xlim(0, 1)</code></pre>
<p><img src="mental_files/figure-html/unnamed-chunk-38-1.png" width="480" /> Nevertheless, even after perfect calibration of a classifier, its ROC is not affected and its classification ability remains unchanged. This is discussed in this paper in detail if you are interested: <a href="http://www.stat.wvu.edu/~jharner/courses/dsci503/docs/vuk.pdf" class="uri">http://www.stat.wvu.edu/~jharner/courses/dsci503/docs/vuk.pdf</a></p>
<p>A good example about calibration is discussed also in Nate Silver’s book the Signal and Noise regarding weather forecasts being tweaked a bit on tv channels to predict more rain. It is called the ‘wet bias’. They want to predict more rain than the evidence suggests, not to make people angry.</p>
<p>Thank for surviving until here! See you in the next chapters.</p>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
