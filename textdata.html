<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title></title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/paper.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<link href="site_libs/font-awesome-5.0.13/css/fa-svg-with-js.css" rel="stylesheet" />
<script src="site_libs/font-awesome-5.0.13/js/fontawesome-all.min.js"></script>
<script src="site_libs/font-awesome-5.0.13/js/fa-v4-shims.min.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>

<link rel="stylesheet" href="main.css" type="text/css" />

</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 64px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 69px;
  margin-top: -69px;
}

.section h2 {
  padding-top: 69px;
  margin-top: -69px;
}
.section h3 {
  padding-top: 69px;
  margin-top: -69px;
}
.section h4 {
  padding-top: 69px;
  margin-top: -69px;
}
.section h5 {
  padding-top: 69px;
  margin-top: -69px;
}
.section h6 {
  padding-top: 69px;
  margin-top: -69px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Cagdas Yetkin</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="final.html">final</a>
</li>
<li>
  <a href="textdata.html">Textdata</a>
</li>
<li>
  <a href="AirBnB.html">AirBnB</a>
</li>
<li>
  <a href="mental.html">Mental</a>
</li>
<li>
  <a href="ml.html">Machine Leaning</a>
</li>
<li>
  <a href="dl.html">Deep Leaning</a>
</li>
<li>
  <a href="London_powermeters.html">Time Series</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="http://github.com/cagdasyetkin">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="https://twitter.com/cagdasyetkin">
    <span class="fa fa-twitter fa-lg"></span>
     
  </a>
</li>
<li>
  <a href="https://www.linkedin.com/in/cagdasyetkin/">
    <span class="fa fa-linkedin fa-lg"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">




</div>


<hr />
<hr />
<p>This section consists of 2 Chapters. Chapter 1 talks about the fundamentals of text analysis using famous books from Tolkien, J.R.R Martin, Jack London and Herman Merville. Chapter 2 builds on these concepts and tells a thrilling story which was liked and retwitted by leading data scientists: Julia Silge and David Robinson on Twitter. Chapter 2 has also a Shiny App which lives in a server I fired up on AWS.</p>
<div id="chapter-1" class="section level2">
<h2>Chapter 1</h2>
<div id="introuction-to-unstructure-text-data-analysis-using-lda" class="section level3">
<h3>Introuction to Unstructure Text Data Analysis using LDA</h3>
<p>When we are dealing with a collection of documents, such as all the text files from the Bronze Age Assyrian civilization, we may want to divide them into natural groups like “Trade” and “Warfare”.</p>
<p>LDA is a popular method for fitting such models: (a) It considers each document as a mixture of topics (b) And each topic as a mixture of words.</p>
<p>What does this mean?</p>
<p>Let’s say we have a 2 topic LDA model on ancient Assyrian texts. The first text script has a probability of 85% coming from topic 1, and a probability of 15% coming from topic 2 (a).</p>
<p>In “Warfare” and “Trade” example above, the most frequent words in “warfare” topic can be: [“massacre”, “head”, “enslave”, “burn”], while “Trade” topic may contain: [“amphora”, “cargo”, “pay”, “ship”] (b).</p>
<p>LDA is a method estimating both of these (a and b) at the same time. It is important to realize that this is an unsupervised method. We don’t have these “Trade” and “Warfare” labels at the beginning. And the analyst decides how many topics there will be.</p>
</div>
<div id="how-should-a-a-full-tidy-text-analysis-look-like" class="section level3">
<h3>How should a a full tidy text analysis look like?</h3>
<p>The overall objective is to process our raw data and to arrive at some meaningful insights. The end results are visualized using libraries like ggplot, igrapgh, ggraph. We handle the data wrangling and processing part using a “tidy” approach (heavily using tidyverse, regexp). Converting to and from non-tidy formats is a crucial skill to have.</p>
<p>As a first step it makes sense to calculate some word frequencies by simple counts and tf_idf. It can be pseudo code as follows:</p>
<pre class="r"><code># get_data() &amp; preprocess() %&gt;%
# unnest_tokens() &gt;&gt; tidy text %&gt;%
# anti_join(stopwords) %&gt;%
# summarize, group_by, count(words), tf-idf %&gt;%
# visualize()</code></pre>
<p>Next natural step can be getting some sentiments from the collection of texts we are looking into. Are these negative or positive texts for example? Or which parts or chapters have what kind of sentiments. A summary pseudo code can be:</p>
<pre class="r"><code># decide/find the lexicons you want to use %&gt;%
# inner_join(lexicon) %&gt;%
# group_by and do summaries %&gt;%
# visualize()</code></pre>
<p>We might also want to look at into ngrams which can capture two or three words in a row and do frequency and sentiment analysis on them. Pseudo code can go like:</p>
<pre class="r"><code># unnest_tokens using ngram %&gt;%
# filtering where needed and then apply tf-idf %&gt;%
# visualize %&gt;%
# apply sentiment analysis on ngram this time%&gt;%
# visualize()</code></pre>
<p>In some certain scenarios, we might be interested in topic modeling where we want to divide our text into natural groups. This can be about feeding all the articles from a newspaper into the algorithm, give how many topics we want as an input and expect the machine to give output topics related to “economy”, “politics” and “sports”. It follows a high-level pseudo code:</p>
<pre class="r"><code># do document term matrix %&gt;%
# apply LDA to do get topics %&gt;%
# work on your topics using dplyr tidyr as usual, tidy model %&gt;%
# visualize()</code></pre>
</div>
<div id="how-does-this-work-in-practice-we-will-do-a-short-tidy-text-analysis-where-we-extract-topics-and-explain-why-they-are-good-or-bad" class="section level3">
<h3>How does this work in practice? We will do a short tidy text analysis where we extract topics and explain why they are good or bad:</h3>
<pre class="r"><code>library(stringr)
library(dplyr)
library(tidyr)
library(tidytext)
library(ggplot2)
library(ggthemes)
library(gridExtra)
library(topicmodels)
library(tm)</code></pre>
<p>We will load 2 books from the Game of Thrones series, The Lord of the Rings series and The Hobbit. These are fantasy fiction novels. They all talk about lords, ladies, kings, prices, wars etc. We will see how we can create two natural groups from this collection.</p>
<p>The underlying philosophy is to be able to use this LDA method even when we are not sure what we are looking for.</p>
<p>However, in this below setup we already know that we have 2 authors only, and thus, 2 natural groups. This can get confusing and even useless if we try to extract 4-5 topics out of them.</p>
<pre class="r"><code>#load the books
GoT2 &lt;- readLines(&quot;data/textdata/GoT2.txt&quot;) %&gt;%
  data_frame() %&gt;%
  mutate(title = &quot;A Clash of Kings&quot;) 

GoT3 &lt;- readLines(&quot;data/textdata/GoT3.txt&quot;) %&gt;%
  data_frame() %&gt;%
  mutate(title = &quot;A Storm of Swords&quot;)

lotr &lt;- readLines(&quot;data/textdata/lotr.txt&quot;) %&gt;% #it contains the entire Lord of The Rings series
  data_frame() %&gt;%
  mutate(title = &quot;Lord of the Rings&quot;)

hobbit &lt;- readLines(&quot;data/textdata/hobbit.txt&quot;) %&gt;%
  data_frame() %&gt;%
  mutate(title = &quot;The Hobbit&quot;)

books &lt;- bind_rows(list(GoT2, GoT3, lotr, hobbit)) 
colnames(books) &lt;- c(&quot;text&quot;, &quot;title&quot;)

my_stop_words &lt;- data_frame(word = c(&#39;page&#39;))</code></pre>
<pre class="r"><code># divide into documents, each representing one chapter
by_chapter &lt;- books %&gt;%
  group_by(title) %&gt;%
  mutate(chapter = cumsum(str_detect(text, regex(&quot;^chapter &quot;, ignore_case = TRUE)))) %&gt;%
  ungroup() %&gt;%
  filter(chapter &gt; 0) %&gt;%
  unite(document, title, chapter)</code></pre>
<pre class="r"><code># split into words
by_chapter_word &lt;- by_chapter %&gt;%
  unnest_tokens(word, text)

# find document-word counts
word_counts &lt;- by_chapter_word %&gt;%
  anti_join(stop_words) %&gt;%
  anti_join(my_stop_words) %&gt;%
  count(document, word, sort = TRUE) %&gt;%
  ungroup()

word_counts</code></pre>
<pre><code>## # A tibble: 239,860 x 3
##    document             word       n
##    &lt;chr&gt;                &lt;chr&gt;  &lt;int&gt;
##  1 A Storm of Swords_80 ser      226
##  2 A Storm of Swords_80 lord     210
##  3 A Storm of Swords_80 son      146
##  4 A Clash of Kings_68  ser      143
##  5 Lord of the Rings_1  frodo    140
##  6 A Clash of Kings_68  lord     129
##  7 Lord of the Rings_1  bilbo    121
##  8 Lord of the Rings_39 pippin   105
##  9 Lord of the Rings_49 sam      105
## 10 A Storm of Swords_80 lady     104
## # ... with 239,850 more rows</code></pre>
<p>‘ser’ means ‘sir’ in the world of Game of Thrones. We see this word ‘ser’ at the top.</p>
<pre class="r"><code>#document term matrix
chapters_dtm &lt;- word_counts %&gt;%
  cast_dtm(document, word, n)</code></pre>
<pre><code>## Warning: Trying to compute distinct() for variables not found in the data:
## - `row_col`, `column_col`
## This is an error, but only a warning is raised for compatibility reasons.
## The operation will return the input unchanged.</code></pre>
<pre class="r"><code>chapters_dtm</code></pre>
<pre><code>## &lt;&lt;DocumentTermMatrix (documents: 224, terms: 23982)&gt;&gt;
## Non-/sparse entries: 239860/5132108
## Sparsity           : 96%
## Maximal term length: 32
## Weighting          : term frequency (tf)</code></pre>
<p>We can call these terms by using the Term() function</p>
<pre class="r"><code>terms &lt;- Terms(chapters_dtm)
head(terms)</code></pre>
<pre><code>## [1] &quot;ser&quot;    &quot;lord&quot;   &quot;son&quot;    &quot;frodo&quot;  &quot;bilbo&quot;  &quot;pippin&quot;</code></pre>
<p>Now it is time to use LDA algorithm to create a 2-topic-model. We know that there are 2 autors as discussed before. In other problems we could try different k values and try to come up with some meaningful results.</p>
<pre class="r"><code>chapters_lda &lt;- LDA(chapters_dtm, k = 2, control = list(seed = 1234))
chapters_lda</code></pre>
<pre><code>## A LDA_VEM topic model with 2 topics.</code></pre>
<pre class="r"><code>#per-topic-per-word probabilities
chapter_topics &lt;- tidy(chapters_lda, matrix = &quot;beta&quot;)
chapter_topics</code></pre>
<pre><code>## # A tibble: 47,964 x 3
##    topic term      beta
##    &lt;int&gt; &lt;chr&gt;    &lt;dbl&gt;
##  1     1 ser   8.82e-14
##  2     2 ser   9.23e- 3
##  3     1 lord  2.17e- 3
##  4     2 lord  1.15e- 2
##  5     1 son   1.03e- 3
##  6     2 son   2.51e- 3
##  7     1 frodo 9.76e- 3
##  8     2 frodo 1.90e-41
##  9     1 bilbo 4.15e- 3
## 10     2 bilbo 6.53e-38
## # ... with 47,954 more rows</code></pre>
<p>We can see that we arrived to a one-topic-per-term-per-row format. We see the probabilities of a term coming from each topic. For example, the term “ser” has almost zero probability of being generated from topic 1, but it has a high probability of coming from topic 2.</p>
<p>Now we will look into top terms in the topics</p>
<pre class="r"><code>top_terms &lt;- chapter_topics %&gt;%
  group_by(topic) %&gt;%
  top_n(5, beta) %&gt;%
  ungroup() %&gt;%
  arrange(topic, -beta)

top_terms</code></pre>
<pre><code>## # A tibble: 10 x 3
##    topic term       beta
##    &lt;int&gt; &lt;chr&gt;     &lt;dbl&gt;
##  1     1 frodo   0.00976
##  2     1 sam     0.00697
##  3     1 gandalf 0.00660
##  4     1 dark    0.00500
##  5     1 time    0.00477
##  6     2 lord    0.0115 
##  7     2 ser     0.00923
##  8     2 tyrion  0.00455
##  9     2 king    0.00448
## 10     2 jon     0.00436</code></pre>
<p>and visualize</p>
<pre class="r"><code>top_terms %&gt;%
  mutate(term = reorder(term, beta)) %&gt;%
  ggplot(aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = &quot;free&quot;) + theme_fivethirtyeight() +
  coord_flip()</code></pre>
<p><img src="textdata_files/figure-html/unnamed-chunk-14-1.png" width="672" /></p>
<p>It is amazing. The heroes and the concepts are separated correctly. The algorithm was able to classify the 2 authors. This is nice. However, I have set the k = 2 myself. I knew that there were 2 autors and topics. We should also keep in mind that the Lord of the Rings was a continuation of the Hobbit.</p>
<p>If these information is not available to us, it would be hard to evaluate and understand. It can also give misleading results.</p>
<p>Each document in this analysis represented a single chapter. Now imagine all the chapters are mixed up and we are trying to figure out which chapter belongs to which autor(topic). Can we do that?</p>
<p>per-document-per-topic probabilities : γ(“gamma”)</p>
<pre class="r"><code>chapters_gamma &lt;- tidy(chapters_lda, matrix = &quot;gamma&quot;)
chapters_gamma</code></pre>
<pre><code>## # A tibble: 448 x 3
##    document             topic      gamma
##    &lt;chr&gt;                &lt;int&gt;      &lt;dbl&gt;
##  1 A Storm of Swords_80     1 0.00000533
##  2 A Clash of Kings_68      1 0.00000953
##  3 Lord of the Rings_1      1 1.000     
##  4 Lord of the Rings_39     1 1.000     
##  5 Lord of the Rings_49     1 1.000     
##  6 A Storm of Swords_67     1 0.0000236 
##  7 A Storm of Swords_19     1 0.0000176 
##  8 Lord of the Rings_2      1 1.000     
##  9 Lord of the Rings_10     1 1.000     
## 10 Lord of the Rings_7      1 1.000     
## # ... with 438 more rows</code></pre>
<p>Each gamma you see here is estimated proportion of words from that chapter that are generated from that topic. For example, we estimate that each word in the Storm of Swords Chapter 80 has only 0.000532% probability of coming from topic 1 (and topic 1 is JRR Tolkien, the author of the Lord of the Rings).</p>
<pre class="r"><code>chapters_gamma &lt;- chapters_gamma %&gt;%
  separate(document, c(&quot;title&quot;, &quot;chapter&quot;), sep = &quot;_&quot;, convert = TRUE)

chapters_gamma</code></pre>
<pre><code>## # A tibble: 448 x 4
##    title             chapter topic      gamma
##    &lt;chr&gt;               &lt;int&gt; &lt;int&gt;      &lt;dbl&gt;
##  1 A Storm of Swords      80     1 0.00000533
##  2 A Clash of Kings       68     1 0.00000953
##  3 Lord of the Rings       1     1 1.000     
##  4 Lord of the Rings      39     1 1.000     
##  5 Lord of the Rings      49     1 1.000     
##  6 A Storm of Swords      67     1 0.0000236 
##  7 A Storm of Swords      19     1 0.0000176 
##  8 Lord of the Rings       2     1 1.000     
##  9 Lord of the Rings      10     1 1.000     
## 10 Lord of the Rings       7     1 1.000     
## # ... with 438 more rows</code></pre>
<pre class="r"><code># reorder titles in order of topic 1, topic 2, etc before plotting
chapters_gamma %&gt;%
  mutate(title = reorder(title, gamma * topic)) %&gt;%
  ggplot(aes(factor(topic), gamma)) +
  geom_boxplot() +
  facet_wrap(~ title)</code></pre>
<p><img src="textdata_files/figure-html/unnamed-chunk-17-1.png" width="672" /> We notice that almost all of the chapters from The Lord of The Rings and The Game of Thrones series were uniquely identified as a single topic each.</p>
<p>Are there any cases where the topic most associated with a chapter belonged to another <strong>autor</strong>?</p>
<pre class="r"><code>chapter_classifications &lt;- chapters_gamma %&gt;%
  group_by(title, chapter) %&gt;%
  top_n(1, gamma) %&gt;%
  ungroup()

chapter_classifications</code></pre>
<pre><code>## # A tibble: 224 x 4
##    title             chapter topic gamma
##    &lt;chr&gt;               &lt;int&gt; &lt;int&gt; &lt;dbl&gt;
##  1 Lord of the Rings       1     1 1.000
##  2 Lord of the Rings      39     1 1.000
##  3 Lord of the Rings      49     1 1.000
##  4 Lord of the Rings       2     1 1.000
##  5 Lord of the Rings      10     1 1.000
##  6 Lord of the Rings       7     1 1.000
##  7 The Hobbit              5     1 1.000
##  8 Lord of the Rings      22     1 1.000
##  9 Lord of the Rings      21     1 1.000
## 10 Lord of the Rings      51     1 1.000
## # ... with 214 more rows</code></pre>
<p>Find the misclassified chapters</p>
<pre class="r"><code>book_topics &lt;- chapter_classifications %&gt;%
  count(title, topic) %&gt;%
  group_by(title) %&gt;%
  top_n(1, n) %&gt;%
  ungroup() %&gt;%
  transmute(consensus = title, topic)

chapter_classifications %&gt;%
  inner_join(book_topics, by = &quot;topic&quot;) %&gt;%
  filter(title != consensus)</code></pre>
<pre><code>## # A tibble: 224 x 5
##    title             chapter topic gamma consensus        
##    &lt;chr&gt;               &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;            
##  1 Lord of the Rings       1     1 1.000 The Hobbit       
##  2 Lord of the Rings      39     1 1.000 The Hobbit       
##  3 Lord of the Rings      49     1 1.000 The Hobbit       
##  4 Lord of the Rings       2     1 1.000 The Hobbit       
##  5 Lord of the Rings      10     1 1.000 The Hobbit       
##  6 Lord of the Rings       7     1 1.000 The Hobbit       
##  7 The Hobbit              5     1 1.000 Lord of the Rings
##  8 Lord of the Rings      22     1 1.000 The Hobbit       
##  9 Lord of the Rings      21     1 1.000 The Hobbit       
## 10 Lord of the Rings      51     1 1.000 The Hobbit       
## # ... with 214 more rows</code></pre>
<p>It turns out there is chapter misclassification only within the same autor. Otherwise, we classified the autors perfectly.</p>
<p>I was expecting to have misclassification of chapters within the same author. Because these books are continuation of each other: The same heroes and events.</p>
<p>LDA can be a good approach when we have a huge collection of unlabeled texts and we are trying to make sense of it. However, we should not forget that we are setting up how many topics will be generated ourselves.</p>
</div>
<div id="what-is-unnest_token-function-and-how-we-can-use-it" class="section level3">
<h3>What is unnest_token function and how we can use it:</h3>
<p>It is a function from Tidytext library which restructures text: Creates one token for each row. It splits a text column (this is our input) into tokens (like words). It helps us doing this tokenization.</p>
</div>
<div id="have-you-ever-checked-what-gutenbergr-package-does" class="section level3">
<h3>Have you ever checked what gutenbergr package does:</h3>
<p>Project Gutenberg digitizes the books for which copyright has expired with the help of volunteers. Gutenbergr R package provides these books to R users. We can download and process these books using this library.</p>
</div>
<div id="how-does-a-sentiment-lexicon-work" class="section level3">
<h3>How does a sentiment lexicon work:</h3>
<p>They are like dictionaries which matches words with their sentiment or emotion. Such as classifying them into Positive - Negative - Neutral categories. Once we match the words in our text with lexicon, we can start analyzing the frequencies. Even if we dont know the language in which the text has been written, we can have an overall understanding.</p>
</div>
<div id="why-inner_join-is-important-to-us-here" class="section level3">
<h3>Why inner_join is important to us here:</h3>
<p>We match the words in our text with the sentiments in the lexicon. There can be lots of words which are not available in the lexicon. Similarly, there can be lots of words in the lexicon which are not mentioned in our text. inner_join brings us the intersection between our text and the lexicon. So that we can go ahead with our analysis with the words we have in the lexicon.</p>
<pre class="r"><code>#tidy_books %&gt;%
#  filter(book == &quot;Emma&quot;) %&gt;%
#  inner_join(nrcjoy) %&gt;%
#  count(word, sort = TRUE)</code></pre>
</div>
<div id="what-tf-idf-algorithm-does" class="section level3">
<h3>what tf-idf algorithm does:</h3>
<p>It is a heuristic approach which tells us how importand a word is in the text we are analyzing. It computes the frequencies (tf) and adds a tweak(idf). This tweak is about how rarely that word is used: It reduces the importance of a word used many times in the text and increases the importance of a word not used that much.</p>
</div>
<div id="why-do-we-want-to-do-tokenization-by-bigram" class="section level3">
<h3>Why do we want to do tokenization by bigram:</h3>
<p>If I am trying to capture the right sentiment then I may use bigram.</p>
<p>After removing the stopwords I may get a list of high frequency words like ‘good’ ‘nice’. However if these are used together with the word ‘not’ then in fact these are negative phrases: ‘not good’, ‘not nice’. It would be a critical error if I dont look into this.</p>
<p>Now let’s get our hands a bit dirty with these. Pick two or more authors that you are familiar with, download their texts using the gutenbergr package, and do a basic analysis of word frequencies and TF-IDF</p>
<pre class="r"><code>library(gutenbergr)
library(stringr)
library(dplyr)
library(tidytext)
library(ggplot2)
library(ggthemes)
library(gridExtra)
library(scales)</code></pre>
<pre><code>## 
## Attaching package: &#39;scales&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:psych&#39;:
## 
##     alpha, rescale</code></pre>
<pre class="r"><code>gutenberg_works(str_detect(author, &quot;Herman&quot;)) %&gt;%
  select(gutenberg_id, title) %&gt;%
  head()</code></pre>
<pre><code>## # A tibble: 6 x 2
##   gutenberg_id title                             
##          &lt;int&gt; &lt;chr&gt;                             
## 1           15 Moby Dick                         
## 2         1900 Typee: A Romance of the South Seas
## 3         2489 Moby Dick; Or, The Whale          
## 4         2500 Siddhartha                        
## 5         2694 I and My Chimney                  
## 6         4045 Omoo: Adventures in the South Seas</code></pre>
<p>Get the metadata</p>
<pre class="r"><code>meta &lt;- as.tbl(gutenberg_metadata)
names(meta)</code></pre>
<pre><code>## [1] &quot;gutenberg_id&quot;        &quot;title&quot;               &quot;author&quot;             
## [4] &quot;gutenberg_author_id&quot; &quot;language&quot;            &quot;gutenberg_bookshelf&quot;
## [7] &quot;rights&quot;              &quot;has_text&quot;</code></pre>
<p>Find another way to see Moby Dick and White Fang novels</p>
<pre class="r"><code>meta %&gt;%
    filter(author == &quot;Melville, Herman&quot;,
           language == &quot;en&quot;,
           gutenberg_id == 2489,
           has_text,
           !str_detect(rights, &quot;Copyright&quot;)) %&gt;%
           distinct(title, gutenberg_id)</code></pre>
<pre><code>## # A tibble: 1 x 2
##   gutenberg_id title                   
##          &lt;int&gt; &lt;chr&gt;                   
## 1         2489 Moby Dick; Or, The Whale</code></pre>
<pre class="r"><code>meta %&gt;%
  filter(author == &quot;London, Jack&quot;,
         language == &quot;en&quot;,
         title == &#39;White Fang&#39;,
         has_text,
         !str_detect(rights, &quot;Copyright&quot;)) %&gt;%
         distinct(title, gutenberg_id)</code></pre>
<pre><code>## # A tibble: 1 x 2
##   gutenberg_id title     
##          &lt;int&gt; &lt;chr&gt;     
## 1          910 White Fang</code></pre>
<p>Download the best books from Jack London and Herman Melville</p>
<pre class="r"><code>LondonBooks &lt;- gutenberg_download(c(910, 215, 1164))
MervilleBooks &lt;- gutenberg_download(c(2500, 2489, 1900))</code></pre>
<p>Convert them to Tidy format</p>
<pre class="r"><code>tidy_London &lt;- LondonBooks %&gt;%
  unnest_tokens(word, text) %&gt;%
  anti_join(stop_words)

tidy_Merville &lt;- MervilleBooks %&gt;%
  unnest_tokens(word, text) %&gt;%
  anti_join(stop_words)</code></pre>
<p>Have a look at the top 10 words in these books</p>
<pre class="r"><code>tidy_London %&gt;%
  count(word, sort = TRUE) %&gt;%
  head(10)</code></pre>
<pre class="r"><code>tidy_Merville %&gt;%
  count(word, sort = TRUE) %&gt;%
  head(10)</code></pre>
<p>It doesnt surprise me to see “Whale” and “White Fang” on the top. The wolf and the the whale are both natural hunters in the wild these autors wrote about.</p>
<pre class="r"><code>p1 &lt;- tidy_London %&gt;%
          count(word, sort = TRUE) %&gt;%
          filter(n &gt; 250) %&gt;%
          mutate(word = reorder(word, n)) %&gt;%
          ggplot(aes(word, n)) + geom_col() + xlab(NULL) + coord_flip() +
          ggtitle(&quot;Jack London&quot;) +
          theme_fivethirtyeight()

p2 &lt;- tidy_Merville %&gt;%
          count(word, sort = TRUE) %&gt;%
          filter(n &gt; 250) %&gt;%
          mutate(word = reorder(word, n)) %&gt;%
          ggplot(aes(word, n)) + geom_col() + xlab(NULL) + coord_flip() + 
          ggtitle(&quot;Herman Merville&quot;) +
          theme_fivethirtyeight()


grid.arrange(p1, p2, ncol=2)</code></pre>
<p><img src="textdata_files/figure-html/unnamed-chunk-30-1.png" width="672" /></p>
<pre class="r"><code>library(tidyr)

frequency &lt;- bind_rows(mutate(tidy_Merville, author = &quot;Herman Merville&quot;), 
                       mutate(tidy_London, author = &quot;Jack London&quot;)) %&gt;% 
  
  mutate(word = str_extract(word, &quot;[a-z&#39;]+&quot;)) %&gt;%
  count(author, word) %&gt;%
  group_by(author) %&gt;%
  mutate(proportion = n / sum(n)) %&gt;% 
  select(-n) %&gt;% 
  spread(author, proportion) %&gt;% 
  gather(author, proportion, `Herman Merville`)</code></pre>
<pre class="r"><code>ggplot(frequency, aes(x = proportion, y = `Jack London`, color = abs(`Jack London` - proportion))) +
  geom_abline(color = &quot;gray40&quot;, lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  theme(legend.position=&quot;none&quot;) +
  labs(y = &quot;Jack London&quot;, x = NULL)</code></pre>
<p><img src="textdata_files/figure-html/unnamed-chunk-32-1.png" width="864" /></p>
<p>One guy is focusing on the wolves (fang meaning wolf) and the other is talking about the whales in general, nicely visible in the plot. Both are the most clever hunters in the wild. One ruling the oceans and the other the land. Wolf and Whale make our autors different.</p>
<p>We see that they are sharing quite a lot words. These are scattered around the 45 degree line. Lets see the correlation score.</p>
<pre class="r"><code>cor.test(data = frequency[frequency$author == &quot;Herman Merville&quot;,],
         ~ proportion + `Jack London`)</code></pre>
<pre><code>## 
##  Pearson&#39;s product-moment correlation
## 
## data:  proportion and Jack London
## t = 50.568, df = 8056, p-value &lt; 2.2e-16
## alternative hypothesis: true correlation is not equal to 0
## 95 percent confidence interval:
##  0.4741043 0.5072557
## sample estimates:
##       cor 
## 0.4908577</code></pre>
<p>There is correlation but not so high…</p>
<p>Now lets see tf-idf to see the most important words in these books.</p>
<pre class="r"><code>LondonBooks &lt;- gutenberg_download(c(910, 215, 1164), meta_fields = &quot;title&quot;)
MervilleBooks &lt;- gutenberg_download(c(2500, 2489, 1900), meta_fields = &quot;title&quot;)

LondonBooks %&gt;%
  count(title)</code></pre>
<pre><code>## # A tibble: 3 x 2
##   title                    n
##   &lt;chr&gt;                &lt;int&gt;
## 1 The Call of the Wild  3031
## 2 The Iron Heel         9605
## 3 White Fang            7266</code></pre>
<pre class="r"><code>MervilleBooks %&gt;%
  count(title)</code></pre>
<pre><code>## # A tibble: 3 x 2
##   title                                  n
##   &lt;chr&gt;                              &lt;int&gt;
## 1 Moby Dick; Or, The Whale           23571
## 2 Siddhartha                          3921
## 3 Typee: A Romance of the South Seas 11183</code></pre>
<pre class="r"><code>London_words &lt;- LondonBooks %&gt;%
  unnest_tokens(word, text) %&gt;%
  count(title, word, sort = TRUE) %&gt;%
  ungroup()

London_total_words &lt;- London_words %&gt;% 
  group_by(title) %&gt;% 
  summarize(total = sum(n))

London_words &lt;- left_join(London_words, London_total_words)

Merville_words &lt;- MervilleBooks %&gt;%
  unnest_tokens(word, text) %&gt;%
  count(title, word, sort = TRUE) %&gt;%
  ungroup()

Merville_total_words &lt;- Merville_words %&gt;% 
  group_by(title) %&gt;% 
  summarize(total = sum(n))

Merville_words &lt;- left_join(Merville_words, Merville_total_words)</code></pre>
<pre class="r"><code>London_words &lt;- London_words %&gt;%
  bind_tf_idf(word, title, n)
head(London_words)</code></pre>
<pre><code>## # A tibble: 6 x 7
##   title                word      n total     tf   idf tf_idf
##   &lt;chr&gt;                &lt;chr&gt; &lt;int&gt; &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;
## 1 The Iron Heel        the    6539 88675 0.0737     0      0
## 2 White Fang           the    5148 72920 0.0706     0      0
## 3 The Iron Heel        and    3407 88675 0.0384     0      0
## 4 The Iron Heel        of     3319 88675 0.0374     0      0
## 5 White Fang           and    3004 72920 0.0412     0      0
## 6 The Call of the Wild the    2283 32121 0.0711     0      0</code></pre>
<pre class="r"><code>Merville_words &lt;- Merville_words %&gt;%
  bind_tf_idf(word, title, n)
head(Merville_words)</code></pre>
<pre><code>## # A tibble: 6 x 7
##   title                             word      n  total     tf   idf tf_idf
##   &lt;chr&gt;                             &lt;chr&gt; &lt;int&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;
## 1 Moby Dick; Or, The Whale          the   14440 216060 0.0668     0      0
## 2 Typee: A Romance of the South Se~ the    8564 114725 0.0746     0      0
## 3 Moby Dick; Or, The Whale          of     6603 216060 0.0306     0      0
## 4 Moby Dick; Or, The Whale          and    6428 216060 0.0298     0      0
## 5 Typee: A Romance of the South Se~ of     5093 114725 0.0444     0      0
## 6 Moby Dick; Or, The Whale          a      4716 216060 0.0218     0      0</code></pre>
<pre class="r"><code>London_words %&gt;%
  select(-total) %&gt;%
  arrange(desc(tf_idf)) %&gt;%
  head()</code></pre>
<pre><code>## # A tibble: 6 x 6
##   title                word         n      tf   idf  tf_idf
##   &lt;chr&gt;                &lt;chr&gt;    &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;
## 1 The Call of the Wild buck       313 0.00974 0.405 0.00395
## 2 The Iron Heel        ernest     318 0.00359 1.10  0.00394
## 3 The Call of the Wild thornton    81 0.00252 1.10  0.00277
## 4 White Fang           grey       150 0.00206 1.10  0.00226
## 5 The Call of the Wild spitz       60 0.00187 1.10  0.00205
## 6 The Iron Heel        labor      163 0.00184 1.10  0.00202</code></pre>
<pre class="r"><code>Merville_words %&gt;%
  select(-total) %&gt;%
  arrange(desc(tf_idf)) %&gt;%
  head()</code></pre>
<pre><code>## # A tibble: 6 x 6
##   title                              word          n      tf   idf  tf_idf
##   &lt;chr&gt;                              &lt;chr&gt;     &lt;int&gt;   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;
## 1 Siddhartha                         siddhart~   374 0.00952 1.10  0.0105 
## 2 Siddhartha                         govinda     140 0.00356 1.10  0.00391
## 3 Typee: A Romance of the South Seas kory        236 0.00206 1.10  0.00226
## 4 Moby Dick; Or, The Whale           whale      1094 0.00506 0.405 0.00205
## 5 Siddhartha                         kamala       72 0.00183 1.10  0.00201
## 6 Typee: A Romance of the South Seas toby        189 0.00165 1.10  0.00181</code></pre>
<pre class="r"><code>p3 &lt;- London_words %&gt;%
        arrange(desc(tf_idf)) %&gt;%
        mutate(word = factor(word, levels = rev(unique(word)))) %&gt;% 
        group_by(title) %&gt;% 
        top_n(10) %&gt;% 
        ungroup %&gt;%
        ggplot(aes(word, tf_idf, fill = title)) +
        geom_col(show.legend = FALSE) +
        labs(x = NULL, y = &quot;tf-idf&quot;) +
        facet_wrap(~title, ncol = 2, scales = &quot;free&quot;) +
        theme_fivethirtyeight() + ggtitle(&quot;Jack London&quot;) +
        coord_flip()

p4 &lt;- Merville_words %&gt;%
        arrange(desc(tf_idf)) %&gt;%
        mutate(word = factor(word, levels = rev(unique(word)))) %&gt;% 
        group_by(title) %&gt;% 
        top_n(10) %&gt;% 
        ungroup %&gt;%
        ggplot(aes(word, tf_idf, fill = title)) +
        geom_col(show.legend = FALSE) +
        labs(x = NULL, y = &quot;tf-idf&quot;) +
        facet_wrap(~title, ncol = 2, scales = &quot;free&quot;) +
        theme_fivethirtyeight() + ggtitle(&quot;Herman Merville&quot;) +
        coord_flip()

grid.arrange(p3, p4, nrow=2)</code></pre>
<p><img src="textdata_files/figure-html/unnamed-chunk-38-1.png" width="768" /></p>
It is interesting to see the high frequency words we plotted in the first part disappeared after we applied tf_idf.
</p>
White, Fang, Whale, Sea, Boat are all gone. This is in the nature of tf-idf algorithm: decreases the weight for commonly used words and increases the weight for words that are not used very much.
</p>
<p>Now we completed the first chapter. If you followed along you already know how to do your own analysis by now. Congrats!</p>
<hr />
<hr />
<style> body { text-align: justify} </style>
</div>
</div>
<div id="chapter-2" class="section level2">
<h2>Chapter 2</h2>
<div id="i-analyze-a-large-dataset-of-commercial-records-produced-by-assyrian-merchants-in-the-19th-century-bce" class="section level3">
<h3>I analyze a large dataset of commercial records produced by Assyrian merchants in the 19th Century BCE…</h3>
<p>Allright, I don’t have any history, archeology or any social science background other than watching Indiana Jones and playing Lara Croft’s Tomb Raider. On the other hand, I did some google search to give you the context and placed them here.</p>
<p>The aim of the project is to make sense of some text which I have no idea about. However, If you give some feedback I will be happy to update the post.</p>
<p>This is about an ancient language from the Bronze Age, meaning like 4000 years ago.</p>
<p><a href="https://soundcloud.com/user444756202/the-epic-of-gilgames-standard-version-tablet-xi-lines-1-163-read-by-karl-hecker" target="_blank" title="from Gilgamesh">this is how the language sounded like…</a></p>
</div>
<div id="and-this-is-how-it-looked-like" class="section level3">
<h3>And this is how it looked like:</h3>
<p><img src="images/textdata_images/sample.PNG" alt="sample" height="200" width="300"></p>
</div>
<div id="some-smart-people-decrypted-these-languages.-for-example" class="section level3">
<h3>Some smart people decrypted these languages. For example</h3>
<p>nu ninda en e-iz-za-te-ni </n> <b>wa-a-tar</b>-ma e-ku-ut-te-ni</p>
<p>Translation:</p>
<p>“Now you will eat bread and drink <strong>water</strong>”</p>
<p>wa-a-tar means water. ninda means bread. Quite familiar isn’t it?</p>
</div>
<div id="our-data-looks-like-this" class="section level3">
<h3>Our data looks like this:</h3>
<p>This is a txt file.</p>
<p><img src="images/textdata_images/sample2.PNG" alt="data" height="350" width="550"></p>
<p>But how did I get here? It started with a tweet by Gabor Bekes:</p>
<p><img src="images/textdata_images/tweetgabor.PNG" alt="data" height="300" width="500"></p>
<p>I contacted the authors of this academic paper from the University of Virginia. They were so kind and provided me these text files:</p>
<p><img src="images/textdata_images/kerememail.PNG" alt="data" height="300" width="500"></p>
<p>Archeologists have escavated these tablets during the past 200 years.</p>
</div>
<div id="an-excavated-tablet-looks-like-this" class="section level3">
<h3>An excavated tablet looks like this:</h3>
<p><img src="images/textdata_images/sample3.png" alt="tablet" height="200" width="200"></p>
</div>
<div id="these-people-lived-4000-years-ago-in-the-middle-east-and-the-asia-minor" class="section level3">
<h3>These people lived 4000 years ago in the Middle East and the Asia Minor</h3>
<p>We will start by loading our documents.</p>
<pre class="r"><code>#we have a collection of 4 documents
oare1 &lt;- readLines(&quot;data/textdata/OARE_01.txt&quot;) %&gt;%
  data_frame(file = &#39;OARE_1&#39;) %&gt;%
  rename(&#39;word&#39; = &#39;.&#39;) 

oare2 &lt;- readLines(&quot;data/textdata/OARE_02.txt&quot;) %&gt;%
  data_frame(file = &#39;OARE_2&#39;) %&gt;%
  rename(&#39;word&#39; = &#39;.&#39;) 

oare3 &lt;- readLines(&quot;data/textdata/OARE_03.txt&quot;) %&gt;%
  data_frame(file = &#39;OARE_3&#39;) %&gt;%
  rename(&#39;word&#39; = &#39;.&#39;) 

oare4 &lt;- readLines(&quot;data/textdata/OARE_04.txt&quot;) %&gt;%
  data_frame(file = &#39;OARE_4&#39;) %&gt;%
  rename(&#39;word&#39; = &#39;.&#39;) 

txt &lt;- bind_rows(list(oare1, oare2, oare3, oare4)) 
colnames(txt) &lt;- c(&quot;text&quot;, &quot;title&quot;)</code></pre>
<p>How many tablets do we have? Each record has an original language indicator “logosyllabic, Akkadian”. We can simply count them.</p>
<pre class="r"><code>str_c(&quot;We have &quot;, nrow(txt[grep(&#39;^logosyllabic&#39;, txt$text),]), &quot; tablets&quot;) %&gt;%
  pander::pander()</code></pre>
<p>We have 689 tablets</p>
<p>We have many unwanted words and phrases. Some simple regular expressions will help us.</p>
<pre class="r"><code>#see the lines give the information regarding the Creator of the record
head(txt[grep(&#39;^Creator&#39;, txt$text),], 3)</code></pre>
<pre><code>## # A tibble: 3 x 2
##   text                                             title 
##   &lt;chr&gt;                                            &lt;chr&gt; 
## 1 Creator(s): Stratford, Edward P.                 OARE_1
## 2 Creator(s): Stratford, Edward P.                 OARE_1
## 3 &quot;Creator(s): Stratford, Edward P. (2016-12-24) &quot; OARE_1</code></pre>
<pre class="r"><code>#removing them
txt &lt;- txt %&gt;%
  anti_join(txt[grep(&#39;^Creator&#39;, txt$text),])</code></pre>
<p>This is one way of cleaning such unwanted words… We can create a list of them, match and then anti_join easily.</p>
<p>toMatch is a vector of unwanted character strings with some regular expressions just like above example. We remove them from our text below.</p>
<pre class="r"><code>library(pander)
matches &lt;- unique (grep(paste(toMatch,collapse=&quot;|&quot;), 
                        txt$text, value=TRUE))

#removing them
txt &lt;- txt %&gt;%
  anti_join(data.frame(text = matches)) 

original_text &lt;- txt # keep an untouched copy

#take a look at one commercial record now
pander(txt[149:150, 1])</code></pre>
<table style="width:43%;">
<colgroup>
<col width="43%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">text</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">4-6I entrusted 15 minas 54 shekels silver under my seal, 16 shekels its shipping charge, to Assur-samsi on behalf of Al-ili.</td>
</tr>
<tr class="even">
<td align="center">7-8There, make sure the representative of Al-ili recieves the silver and it’s shipping charge. 9-11I gave 5 1/2 minas silver of my own to Assur-samsi for purchases.</td>
</tr>
</tbody>
</table>
<p>Here we are reading a business letter from a guy called Abela to Idnaya. Just a quick look makes it clear that these people were not savages at all. Freight charges, clearing a debt on behalf of another entity and stamps (seals) are all in place.</p>
<p>The paragraph is almost clean. Next, we need to remove the numbers.</p>
<pre class="r"><code># Remove numbers using removeNumber function from tm library
library(tm)

txt$text &lt;- txt$text %&gt;%
  removeNumbers() %&gt;%
  removePunctuation() %&gt;%
  tolower() %&gt;% #in fact unnest_tokens function handling this
  stripWhitespace()</code></pre>
<p>Now let’s have a quick look at the word frequencies</p>
<pre class="r"><code>txt %&gt;%
  unnest_tokens(word, text) %&gt;%
  anti_join(stop_words) %&gt;%
  count(word, sort = TRUE) %&gt;%
  filter(n &gt; 150) %&gt;%
  mutate(word = reorder(word, n)) %&gt;%
  ggplot(aes(word, n)) + geom_col() + xlab(NULL) + coord_flip() +
  ggtitle(&quot;Word Frequency&quot;)</code></pre>
<p><img src="textdata_files/figure-html/unnamed-chunk-46-1.png" width="672" /></p>
<p>It looks like these people were sending and receiving textiles, tin, and copper. Transportation was mostly depending on donkeys and the payments were in silvers most of the time. The units of measurement (like the weight) were shekels, minas, and talents. Example: Abele sends 5 talents of gold to Ibalaya for 200 shekels of silver, on behalf of Iddin-Istar.</p>
<p>Looking at the frequency plot again, what does <strong>pusuken</strong> mean? It is repeating more than 500 times…</p>
<pre class="r"><code>head(original_text[str_detect(original_text$text, &quot;Pusu-ken&quot;), 1], 4) %&gt;%
  pander()</code></pre>
<table style="width:44%;">
<colgroup>
<col width="44%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">text</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">1-3To Amur-Istar and Pusu-ken from Kuzallum:</td>
</tr>
<tr class="even">
<td align="center">1-2From su-hubur to Pusu-ken:</td>
</tr>
<tr class="odd">
<td align="center">1-7Zupa seized us against the sons of Pusu-ken and Zupa said to the sons of Pusu-ken, “Your father took the silver stated in my sealed tablets.</td>
</tr>
<tr class="even">
<td align="center">7-9Why is it that Suen-re’i detains me? 10-18The sons of Pusu-ken (responded): “Our father received the silver of the sealed tablet of Zupa. Release it and we will give you the matter.”</td>
</tr>
</tbody>
</table>
<p>A family business! It turns out Pusuken was the most famous businessman. Probably he was the leader of a powerful family controlling most of the trade happening in the region. His sons were handling the transactions. Assurnada and Assurmalik are the other two notable tradesmen coming out from the frequency table.</p>
<p>Another frequent word is ‘son’. Let’s find out why.</p>
<pre class="r"><code>tail(original_text[str_detect(original_text$text, &quot;son&quot;), 1], 5)[c(1, 3), ] %&gt;%
  pander()</code></pre>
<table style="width:43%;">
<colgroup>
<col width="43%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">text</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Iddin-Istar son of Assur-nada owes 5 5/6</td>
</tr>
<tr class="even">
<td align="center">15-18Witnesses: Burqanim son of Kakkabanim son of su-Istar son of suaya</td>
</tr>
</tbody>
</table>
<p>son of Assurnada, son of Inahili, son of Kakkabanim… Of course, these are family names. That’s why it repeats a lot.</p>
<p>Now we have a fairly good understanding of the most frequent words. Let’s get a big picture of the ancient cities mentioned in our text.</p>
<p>Ancient city names are easily available online. I created a dataframe called “Cities” which contain their geolocations. We will join information into this dataframe from our text. Our objective is to come up with some metric which can represent the trading power of these cities, and visualize them.</p>
<pre class="r"><code>library(tidyr)

cities &lt;- cities %&gt;%
  left_join(original_text %&gt;%
                unnest_tokens(word, text) %&gt;%
                filter(word %in% tolower(cities$cityName)) %&gt;%
                count(word, sort = TRUE) %&gt;%
                rename(cityName = word),
            
            by = &#39;cityName&#39;)

cities$n &lt;- replace_na(cities$n, 0)</code></pre>
<pre class="r"><code>cities &lt;- cities %&gt;%
  select(cityName, latitude, longitude, n) %&gt;%
  rename(Occurance_In_Text = n) %&gt;%
  arrange(desc(Occurance_In_Text))

#cities to plot
cities_in_text &lt;- cities %&gt;%
  filter(Occurance_In_Text &gt; 0) </code></pre>
<p>We can create a data structure to understand the co occurence of these cities with words like “to” and “from” in each line. It can give us a <strong>naive</strong> estimate of goods and services coming in and out. “to” stands for import, “from” stands for export, the summation of “in”+“at” stands for inventory.</p>
<div id="trading-power-export-inventory---import" class="section level4">
<h4>Trading Power = <strong>EXPORT</strong> + <strong>INVENTORY</strong> - <strong>IMPORT</strong></h4>
<pre class="r"><code>pointer &lt;- c(&quot;to&quot;, &quot;from&quot;, &quot;in&quot;, &quot;at&quot;)

for(j in 1:length(pointer)) {

      for(i in 1:length(cities_in_text$cityName)) {
            
                cities_in_text[i, pointer[j]] &lt;- nrow(txt[grep(
                      
                                str_c(pointer[j], &#39; &#39;, cities_in_text$cityName[i]), 
                                                     
                                txt$text), ])}
}


cities_in_text &lt;- cities_in_text %&gt;%
                    mutate(import = to,
                           export = from,
                           inventory = `in` + at,
                           trade_power = export + inventory - import) %&gt;%
                    select(cityName, Occurance_In_Text, trade_power, latitude, longitude) %&gt;%
                    arrange(desc(trade_power))

cities_in_text %&gt;%
  select(cityName, Occurance_In_Text, trade_power) %&gt;%
  head() %&gt;%
  pander::pander()</code></pre>
<table style="width:62%;">
<colgroup>
<col width="16%" />
<col width="27%" />
<col width="18%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">cityName</th>
<th align="center">Occurance_In_Text</th>
<th align="center">trade_power</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">kanesh</td>
<td align="center">87</td>
<td align="center">34</td>
</tr>
<tr class="even">
<td align="center">hahhum</td>
<td align="center">15</td>
<td align="center">8</td>
</tr>
<tr class="odd">
<td align="center">wahsusana</td>
<td align="center">31</td>
<td align="center">7</td>
</tr>
<tr class="even">
<td align="center">zalpa</td>
<td align="center">7</td>
<td align="center">7</td>
</tr>
<tr class="odd">
<td align="center">durhumit</td>
<td align="center">27</td>
<td align="center">6</td>
</tr>
<tr class="even">
<td align="center">tuhpiya</td>
<td align="center">10</td>
<td align="center">5</td>
</tr>
</tbody>
</table>
<pre class="r"><code>library(ggmap)
library(ggthemes)

#setup map borders and select your map style
lat &lt;- cities$latitude
long &lt;- cities$longitude
bbox &lt;- make_bbox(long,lat,f=0.5)
b &lt;- get_map(bbox,maptype=&quot;watercolor&quot;,source=&quot;google&quot;)

#plot the cities mentioned in the text
ggmap(b) + geom_point(data = cities_in_text, 
           aes(longitude, latitude, size=trade_power),alpha=0.2, show.legend = F) +
           scale_size_continuous(range = c(2, 20)) + #for visualization purposes
  
           labs(x = &quot;Longitude&quot;, y = &quot;Latitude&quot;,
           title=&quot;Cities and their Trading Power&quot;, color = &quot;Type&quot;) +
  
           geom_text(data = cities_in_text, aes(longitude, latitude, label = cityName), 
                     check_overlap = TRUE) + 
theme_gdocs()</code></pre>
<p><img src="textdata_files/figure-html/unnamed-chunk-53-1.png" width="768" /></p>
<p>You are looking at the Asia Minor, Mesopotamia, the Middle East, Greece and the southeast Mediterranean Sea. The largest islands you see are the island of Crete and Cyprus.</p>
<p>The city of Kanesh is our biggest hub. Probably a capital city. And our famous Pusuken family have lived here.</p>
<p>The people who wrote these tablets had access to the sea ports in the south. So we are talking about some complicated trade network here. Probably Pharaohs of ancient Egypt, Phonecians and Mycenaeans are all playing a role at some point.</p>
<p>I fired up a shiny app as well, you can freely play around. Please click <a href="http://34.241.213.133:3838/Assur/" target="_blank" title="shinyapplink"><strong>HERE</strong></a> to view it (It is a micro instance, I hope it can stay stable).</p>
<p>We know the most frequent words in these documents. How about the most important ones? We can try tfidf here.</p>
<pre class="r"><code>oare_words &lt;- txt %&gt;%
  unnest_tokens(word, text) %&gt;%
  count(title, word, sort = TRUE) %&gt;%
  ungroup()

total_words &lt;- oare_words %&gt;% 
  group_by(title) %&gt;% 
  summarize(total = sum(n))

oare_words &lt;- left_join(oare_words, total_words, by = &#39;title&#39;)</code></pre>
<pre class="r"><code>oare_stop_words &lt;- data.frame(word = c(&quot;gkt&quot;, &quot;sec&quot;, &quot;oaa&quot;, &quot;ca&quot;, &quot;g&quot;), lexicon = &#39;oare&#39;)

oare_words &lt;- oare_words %&gt;%
  anti_join(oare_stop_words) %&gt;%
  bind_tf_idf(word, title, n)

oare_words %&gt;%
  select(-total) %&gt;%
  arrange(desc(tf_idf)) %&gt;%
  head(10) %&gt;%
  select(title, word, n, tf_idf)</code></pre>
<pre class="r"><code>oare_words %&gt;%
  arrange(desc(tf_idf)) %&gt;%
  mutate(word = factor(word, levels = rev(unique(word)))) %&gt;% 
  top_n(5) %&gt;% 
  ggplot(aes(word, tf_idf, fill = title)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = &quot;tf-idf&quot;) +
  coord_flip() + theme_gray()</code></pre>
<p><img src="textdata_files/figure-html/unnamed-chunk-56-1.png" width="576" /></p>
<p>tf-idf algorithm decreases the weight of commonly used words and increases the weight of words that are not used very much.</p>
<p>We had ruling family names in the word frequency plot. They were the <strong>Pusuken, Assurnada, and Assurmalik</strong> families.</p>
<p>Now we have some other names being reported in tfidf plots. And by far, the most significant names are <strong>Kuliya and Aliabum</strong>. These names are more important than the powerful families according to the algorithm. They are also more important than gold, silver and the trade items. Maybe they are not mentioned that much. But they seem to play some significant role. This is really curious. Who are these people?</p>
<pre class="r"><code>head(original_text[str_detect(original_text$text, &quot;Kuliya&quot;), 1], 15)[4, ] %&gt;%
  pander::pander()</code></pre>
<table style="width:44%;">
<colgroup>
<col width="44%" />
</colgroup>
<thead>
<tr class="header">
<th align="center">text</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">Whoever has brought in loaded donkeys via the narrow track in order to do business over there, even when he is a resident of a karum, make him pay 3 shekels of silver per mina and let Kuliya personally bring it here.</td>
</tr>
</tbody>
</table>
<p>Kuliya is a servant of Pusuken family. He delivers messages, goods, and payments. He is Kuliya, son of Aliabum. Loyal servant to the sons of Pusuken. He takes the caravans from Kanesh and brings to the city of Ulama in the south. There he gives the cargo to the representatives of Sea People. Aliabum is his father. They call him Kuliya, son of Aliabum. This trade network cannot function without him.</p>
</div>
</div>
<div id="an-unusual-autopsy-of-ancient-trade-records" class="section level3">
<h3>An unusual autopsy of ancient trade records</h3>
<pre class="r"><code>library(tidyr)
library(igraph)
library(ggraph)

#create bigram
oare_bigrams &lt;- txt %&gt;%
  unnest_tokens(bigram, text, token = &quot;ngrams&quot;, n = 2)

#separate them
bigrams_separated &lt;- oare_bigrams %&gt;%
  separate(bigram, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;)

#filter out stopwords
bigrams_filtered &lt;- bigrams_separated %&gt;%
  filter(!word1 %in% stop_words$word) %&gt;%
  filter(!word2 %in% stop_words$word) 

#bigram counts:
bigram_counts &lt;- bigrams_filtered %&gt;% 
  count(word1, word2, sort = TRUE)

#create a graph
bigram_graph &lt;- bigram_counts %&gt;%
  filter(n &gt; 30) %&gt;%
  graph_from_data_frame()

#plot the graph

a &lt;- grid::arrow(type = &quot;closed&quot;, length = unit(.15, &quot;inches&quot;))
set.seed(2016)
ggraph(bigram_graph, layout = &quot;fr&quot;) +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, &#39;inches&#39;)) +
  geom_node_point(color = &quot;lightblue&quot;, size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()</code></pre>
<p><img src="textdata_files/figure-html/unnamed-chunk-58-1.png" width="960" /></p>
<p><em>Kuliya wakes up early morning with the sound of donkeys. They are on the way to Ulama.</em></p>
<p><em>The main cargo is 300 kutanum &gt;&gt; textiles. It worths more than 10 talents &gt;&gt; and 50 minas &gt;&gt; of silver according to the tablet he is carrying with him. It means it is almost 2 black &gt;&gt; donkeys! The letter starts with Dear &gt;&gt; brothers again. He doesn’t understand why sons of Pusuken call these bastards brother all the time. Those Sea People could ambush them and loot the cargo. But not this time: A division of Iron Guards from the colony &gt;&gt; office is protecting them.</em></p>
<p><em>Transport &gt;&gt; tariff is 60 shekels &gt;&gt; of silver. They will write a report in Ulama when they arrive, along with an approved delivery note. He will deliver a huge profit to the family to clear all the claims &gt;&gt; outstanding</em></p>
<p>This network plot is quite informative. We see the monetary system around the <strong>silver</strong> and the order of scale for measurement. Talent is the biggest and probably a <strong>refined silver</strong> is the most valuable. Shekels should be like cents. Shekel - Minas - Silver is the strongest triangle of payment. Textile measurements are in kutanum. The strong presence of <strong>colony office</strong> is indicating a regulating superpower, most probably the Empire.</p>
<pre class="r"><code>money &lt;- data.frame(word = c(&quot;tin&quot;, &quot;minas&quot;, &quot;talent&quot;, &quot;talents&quot;, &quot;shekels&quot;, &quot;mina&quot;, &quot;refined&quot;, &quot;silver&quot;))</code></pre>
<p>How do these letters feel like? Mostly <strong>positive</strong> or <strong>negative</strong> language? And which words are contributing most to these sentiments?</p>
<p>I am going to use a special lexicon built for modern financial analysis. This is a kind of lexicon which you can expect to see in an analysis of stock market trading.</p>
<p>I am curious how it can perform on 4000-year-old commercial records.</p>
<pre class="r"><code>library(gridExtra)
#first get the tidy format
tidy_oare &lt;- txt %&gt;%
  group_by(title) %&gt;%
  ungroup() %&gt;%
  unnest_tokens(word, text) %&gt;%
  #remove silver and gold because they dont give sentiment here
  anti_join(data.frame(word = c(&#39;silver&#39;, &#39;gold&#39;))) %&gt;% 
  anti_join(stop_words) %&gt;%
  mutate(linenumber = row_number())
  

#build afinn bing and nrc lexicon sentiments
afinn &lt;- tidy_oare %&gt;% 
  inner_join(get_sentiments(&quot;afinn&quot;)) %&gt;% 
  group_by(index = linenumber %/% 150) %&gt;% 
  summarise(sentiment = sum(score)) %&gt;% 
  mutate(method = &quot;AFINN&quot;)

bing_and_nrc &lt;- bind_rows(tidy_oare %&gt;% 
                            inner_join(get_sentiments(&quot;bing&quot;)) %&gt;%
                            mutate(method = &quot;Bing et al.&quot;),
                          tidy_oare %&gt;% 
                            inner_join(get_sentiments(&quot;nrc&quot;) %&gt;% 
                                         filter(sentiment %in% c(&quot;positive&quot;, 
                                                                 &quot;negative&quot;))) %&gt;%
                            mutate(method = &quot;NRC&quot;)) %&gt;%
  count(method, index = linenumber %/% 150, sentiment) %&gt;%
  spread(sentiment, n, fill = 0) %&gt;%
  mutate(sentiment = positive - negative)

#plot the sentiments together
p1 &lt;- bind_rows(afinn, 
                bing_and_nrc) %&gt;%
        ggplot(aes(index, sentiment, fill = method)) +
        geom_col(show.legend = FALSE) +
        facet_wrap(~method, ncol = 1, scales = &quot;free_y&quot;) + 
        theme_fivethirtyeight() 
        


p2 &lt;- tidy_oare %&gt;%
        count(word) %&gt;%
        inner_join(get_sentiments(&quot;loughran&quot;), by = &quot;word&quot;) %&gt;%
        filter(sentiment %in% c(&quot;positive&quot;, 
                                &quot;negative&quot;,
                                &quot;constrining&quot;,
                                &quot;uncertainty&quot;)) %&gt;% 
        group_by(sentiment) %&gt;%
        top_n(5, n) %&gt;%
        ungroup() %&gt;%
        mutate(word = reorder(word, n)) %&gt;%
        ggplot(aes(word, n)) +
        geom_col() +
        coord_flip() +
        facet_wrap(~ sentiment, scales = &quot;free&quot;) + theme_fivethirtyeight() +
        theme(axis.text.x=element_blank()) + scale_fill_brewer(palette=&quot;Set3&quot;) 

grid.arrange(p1, p2, ncol = 2)</code></pre>
<p><img src="textdata_files/figure-html/unnamed-chunk-60-1.png" width="864" /></p>
<p>Quite many positive sentiments as well as negatives. Not a bad performance by our financial analysis lexicon. Seizing some people or some goods as well as dealing with urgent deliveries must be in Kuliya’s job description (he must have had a strong character and a high spirit). Let’s pick a record about ‘seizing’ and see what it says:</p>
<pre class="r"><code>grep(&quot;.*(kuliya.*seize|seize.*kuliya).*&quot;, txt$text, value = TRUE)[1] %&gt;%
  pander()</code></pre>
<p>as soon as you hear our letter produce the dagger of assur and impose an oath and whoever has sold meteoric iron to a palace or having not yet sold it still carries it with him seize from him the tithe on the meteoric iron and hisits sadduututax of shekels silver per mina and let kuliya personally take it along</p>
<div id="the-chronicles-of-mesopotamia" class="section level4">
<h4><em>The Chronicles of Mesopotamia</em></h4>
<div id="book-1-the-dagger-of-assur" class="section level5">
<h5><em>Book 1: The Dagger of Assur</em></h5>
<div id="chapter-1-meteora" class="section level6">
<h6><em>Chapter 1: Meteora</em></h6>
<pre><code>        *It was before the caravan&#39;s arrival to Kanesh, the city of chaos, when Kuliya got that astounding news...*</code></pre>
<p>Now we can stop here and appreciate for a moment this <a href="https://www.amazon.com/Text-Mining-R-Tidy-Approach/dp/1491981652" target="_blank" title="booklink"><strong>Tidy Approach</strong></a> we followed for Text Mining, bearing in mind the majority of world’s data is unstructured:</p>
<p>We started from knowing almost nothing and got to a point where we can write a fictional story in a Bronze Age Middle East setting.</p>
<p>We were able to understand the building blocks of an ancient trade network and get its sentiments. We have a fairly good understanding of how those operations felt like.</p>
<p>We could identify the trade lords (ruling families) and the most important person in the network who makes this trade engine work. In the end, he became our best friend.</p>
<p>Additionally, we meshed up our text with some information collected from the Internet and built an online dashboard.</p>
<p>The analysis could be even more interesting if we had more collections of text covering more topics like warfare, government, religion.</p>
<p>My <strong>special thanks</strong> to <a href="https://twitter.com/a_kerem_cosar" target="_blank" title="KeremLink">Kerem Cosar</a> from the University of Virginia who accepted sharing these valuable text files. He is one of the authors of the paper; Trade, Merchants, and the Lost Cities of the Bronze Age which made me start typing the first line of code.</p>
<p>Thank you for reading and giving feedback…</p>
</div>
</div>
</div>
</div>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
